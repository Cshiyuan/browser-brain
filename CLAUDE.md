# Browser-Brain é¡¹ç›®æ–‡æ¡£

> åŸºäº Browser-Use AI çš„æ™ºèƒ½æ—…è¡Œè§„åˆ’ç³»ç»Ÿ

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

Browser-Brain æ˜¯ä¸€ä¸ªä½¿ç”¨ AI é©±åŠ¨çš„æ—…è¡Œè§„åˆ’åº”ç”¨ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æ§åˆ¶æµè§ˆå™¨è‡ªåŠ¨çˆ¬å–å°çº¢ä¹¦å’Œæ™¯ç‚¹å®˜ç½‘ä¿¡æ¯ï¼Œç”Ÿæˆä¸ªæ€§åŒ–æ—…è¡Œæ–¹æ¡ˆã€‚

**æ ¸å¿ƒç‰¹æ€§**:
- ğŸ¤– Browser-Use AI æ¡†æ¶é©±åŠ¨çš„æ™ºèƒ½ç½‘é¡µçˆ¬å–
- ğŸŒ Streamlit Web ç•Œé¢
- ğŸ“Š çµæ´»çš„ä¸Šä¸‹æ–‡æ•°æ®æ¨¡å‹è®¾è®¡
- ğŸ”„ å¼‚æ­¥å¹¶å‘çˆ¬å–
- ğŸ¨ æ”¯æŒå¤šç§ LLMï¼ˆOpenAI/Claude/Geminiï¼‰
- ğŸš€ ç‹¬ç«‹æ”¶é›†å™¨æ¨¡å¼ï¼ˆå¯å•ç‹¬è¿è¡Œï¼‰
- ğŸ›¡ï¸ å¢å¼ºåæ£€æµ‹é…ç½®ï¼ˆéšè—è‡ªåŠ¨åŒ–æ ‡è¯†ã€çœŸå® User-Agentï¼‰
- ğŸ” éªŒè¯ç äººå·¥å¤„ç†æœºåˆ¶ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶æš‚åœç­‰å¾…ï¼‰
- âš¡ Fast Mode é€Ÿåº¦ä¼˜åŒ–ï¼ˆå¯é€‰ï¼ŒåŸºäº Browser-Use Fast Agent æŠ€æœ¯ï¼‰
- ğŸ”— Chain Agent Tasksï¼ˆä»»åŠ¡é“¾å¼æ‰§è¡Œï¼Œä¿æŒæµè§ˆå™¨ä¼šè¯ï¼‰
- ğŸš€ Parallel Agentsï¼ˆå¤šæµè§ˆå™¨å¹¶è¡Œæ‰§è¡Œï¼Œæè‡´æ€§èƒ½ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨åº”ç”¨

#### æ–¹å¼ä¸€ï¼šWeb ç•Œé¢ï¼ˆæ¨èï¼‰
```bash
# å¯åŠ¨ Web ç•Œé¢
./run_web.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
source .venv/bin/activate
streamlit run frontend/app.py
```

#### æ–¹å¼äºŒï¼šç‹¬ç«‹æ”¶é›†å™¨
```bash
# å°çº¢ä¹¦æ”¶é›†å™¨ï¼ˆé»˜è®¤æœ‰å¤´æ¨¡å¼ï¼Œæ˜¾ç¤ºæµè§ˆå™¨ï¼‰
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 5

# å®˜ç½‘æ”¶é›†å™¨
./run_official_scraper.sh "åŒ—äº¬æ•…å®«" -l "https://www.dpm.org.cn"
```

**âš ï¸ é‡è¦æç¤º**ï¼š
- **é»˜è®¤ä½¿ç”¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼**ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
- æœ‰å¤´æ¨¡å¼å¯ä»¥ç›´è§‚è§‚å¯Ÿ AI æ“ä½œè¿‡ç¨‹
- ä¾¿äºå‘ç°åçˆ¬è™«ã€éªŒè¯ç ç­‰é—®é¢˜
- åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®ï¼š`HEADLESS=false`ï¼ˆé»˜è®¤å€¼ï¼‰

### çˆ¬è™«æµ‹è¯•

**âš ï¸ æµ‹è¯•è§„åˆ™**ï¼š
- **é»˜è®¤ä½¿ç”¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼**ï¼ˆ`HEADLESS=false`ï¼‰
- **ä¸è¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨è¿›è¡Œæµ‹è¯•**
- æœ‰å¤´æ¨¡å¼å¯ä»¥è§‚å¯Ÿ AI å®é™…æ“ä½œè¿‡ç¨‹
- ä¾¿äºå‘ç°å’Œè°ƒè¯•åçˆ¬è™«é—®é¢˜

```bash
# æµ‹è¯•å°çº¢ä¹¦çˆ¬è™«ï¼ˆ2æ¡ç¬”è®°ï¼Œé»˜è®¤æ˜¾ç¤ºæµè§ˆå™¨ï¼‰
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 2

# æµ‹è¯•å®˜ç½‘çˆ¬è™«ï¼ˆé»˜è®¤æ˜¾ç¤ºæµè§ˆå™¨ï¼‰
./run_official_scraper.sh "åŒ—äº¬æ•…å®«" -l "https://www.dpm.org.cn"

# å¼€å¯ DEBUG æ—¥å¿—æŸ¥çœ‹è¯¦ç»†è¿‡ç¨‹
export LOG_LEVEL=DEBUG
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 2
```

**æµ‹è¯•æµç¨‹**ï¼š
1. è¿è¡Œçˆ¬è™«å‘½ä»¤ï¼ˆé»˜è®¤æ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼‰
2. è§‚å¯Ÿæµè§ˆå™¨çª—å£ä¸­çš„ AI æ“ä½œè¿‡ç¨‹
3. æ£€æŸ¥ç»ˆç«¯æ—¥å¿—è¾“å‡º
4. æŸ¥çœ‹è¿”å›çš„ JSON æ•°æ®
5. æ ¹æ®æ—¥å¿—åˆ†æé—®é¢˜å¹¶ä¿®å¤
6. é‡æ–°æµ‹è¯•éªŒè¯ä¿®å¤æ•ˆæœ

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼ˆå¯é€‰ï¼‰ï¼š
```bash
# å¦‚éœ€æ— å¤´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºæµè§ˆå™¨ï¼‰ï¼Œåœ¨ .env ä¸­è®¾ç½®
HEADLESS=true
```

### ä»£ç æ£€æŸ¥
```bash

# å®Œæ•´æ£€æŸ¥
.venv/bin/pylint app/ --recursive=y

# ä»£ç é£æ ¼æ£€æŸ¥
.venv/bin/pylint app/ --disable=E,R --recursive=y
```

### æ—¥å¿—é…ç½®

é¡¹ç›®ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼š

```bash
# é»˜è®¤ INFO çº§åˆ«
./run_web.sh

# å¼€å¯ DEBUG çº§åˆ«ï¼ˆæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼‰
LOG_LEVEL=DEBUG ./run_web.sh

# ç”Ÿäº§ç¯å¢ƒï¼ˆåªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯ï¼‰
LOG_LEVEL=WARNING ./run_web.sh
```

**æ—¥å¿—ç›®å½•ç»“æ„**ï¼š
```
logs/
â”œâ”€â”€ agents/         # è§„åˆ’å™¨æ—¥å¿—
â”œâ”€â”€ scrapers/       # çˆ¬è™«æ—¥å¿—
â”œâ”€â”€ browser_use/    # Browser-Use AI æ—¥å¿—
â”œâ”€â”€ frontend/       # Web ç•Œé¢æ—¥å¿—
â”œâ”€â”€ models/         # æ•°æ®æ¨¡å‹æ—¥å¿—
â”œâ”€â”€ utils/          # å·¥å…·ç±»æ—¥å¿—
â””â”€â”€ main/           # å…¶ä»–æ—¥å¿—
```

### ç¨‹åºå¯åŠ¨æµç¨‹è¯¦è§£

#### 1. å¯åŠ¨è„šæœ¬æ‰§è¡Œ (`run_web.sh`)

æ‰§è¡Œ `./run_web.sh` åçš„å®Œæ•´æµç¨‹ï¼š

```bash
1. âœ… æ£€æŸ¥å¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
   - ä¼˜å…ˆæŸ¥æ‰¾ .venv ç›®å½•
   - å¤‡ç”¨ venv ç›®å½•
   - è‹¥æ— è™šæ‹Ÿç¯å¢ƒï¼Œè¯¢é—®æ˜¯å¦ä½¿ç”¨ç³»ç»Ÿ Python

2. âœ… æ£€æŸ¥é…ç½®æ–‡ä»¶
   - éªŒè¯ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨
   - è‹¥ä¸å­˜åœ¨ï¼Œæç¤ºå¤åˆ¶ .env.example

3. âœ… éªŒè¯ä¾èµ–
   - æ£€æŸ¥ streamlit æ˜¯å¦å·²å®‰è£…
   - è‹¥æœªå®‰è£…ï¼Œæç¤ºè¿è¡Œ pip install -r requirements.txt

4. âœ… åˆ›å»ºæ•°æ®ç›®å½•
   - åˆ›å»º data/plans ç›®å½•ç”¨äºå­˜å‚¨ç”Ÿæˆçš„æ—…è¡Œæ–¹æ¡ˆ

5. ğŸš€ å¯åŠ¨ Streamlit æœåŠ¡
   streamlit run frontend/app.py --server.port 8501 --server.address localhost
```

**è„šæœ¬ä½ç½®**: `run_web.sh:1-59`

---

#### 5. AI çˆ¬è™«æ‰§è¡Œç»†èŠ‚

**å°çº¢ä¹¦çˆ¬è™«** (app/scrapers/xhs_scraper.py:34-84):
```python
async def search_attraction(self, attraction_name: str, max_notes: int = 5):
    # â‘  æ„å»º AI ä»»åŠ¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
    task = f"""
ä»»åŠ¡ï¼šåœ¨å°çº¢ä¹¦æœç´¢"{attraction_name}"ç›¸å…³çš„æ—…æ¸¸ç¬”è®°

å…·ä½“æ­¥éª¤ï¼š
1. è®¿é—®å°çº¢ä¹¦ç½‘ç«™ https://www.xiaohongshu.com
2. ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½(3-5ç§’)
3. åœ¨æœç´¢æ¡†ä¸­è¾“å…¥å…³é”®è¯ï¼š"{attraction_name}"
4. ç‚¹å‡»æœç´¢æˆ–æŒ‰å›è½¦é”®
5. ç­‰å¾…æœç´¢ç»“æœåŠ è½½å®Œæˆ
6. æµè§ˆæœç´¢ç»“æœï¼Œæ‰¾åˆ°å‰{max_notes}ç¯‡ç›¸å…³ç¬”è®°
7. å¯¹äºæ¯ç¯‡ç¬”è®°ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
   - ç¬”è®°æ ‡é¢˜ã€ä½œè€…ã€æ­£æ–‡å†…å®¹
   - ç‚¹èµæ•°ã€æ”¶è—æ•°ã€è¯„è®ºæ•°
   - ç¬”è®°ä¸­çš„å›¾ç‰‡URLï¼ˆå‰3å¼ ï¼‰
   - æå–ç¬”è®°ä¸­æåˆ°çš„URLé“¾æ¥ï¼ˆç‰¹åˆ«æ˜¯å®˜ç½‘ã€é¢„è®¢ã€é—¨ç¥¨ç›¸å…³é“¾æ¥ï¼‰
   - è¯†åˆ«å…³é”®è¯ï¼ˆå¦‚ï¼šå®˜ç½‘ã€å®˜æ–¹ç½‘ç«™ã€é¢„è®¢ã€é—¨ç¥¨ã€å¼€æ”¾æ—¶é—´ç­‰ï¼‰
    """

    # â‘¡ ä½¿ç”¨ Browser-Use AI æ‰§è¡Œä»»åŠ¡
    result = await self.scrape_with_task(
        task=task,
        output_model=XHSNotesCollection,  # Pydantic æ¨¡å‹å®šä¹‰è¾“å‡ºç»“æ„
        max_steps=30                      # æœ€å¤§æ‰§è¡Œæ­¥éª¤ï¼ˆå°çº¢ä¹¦éœ€è¦å¤šæ­¥æ“ä½œï¼‰
    )

    # â‘¢ è§£æç»“æœå¹¶è½¬æ¢ä¸º XHSNote å¯¹è±¡åˆ—è¡¨
    return xhs_notes
```

**Browser-Use AI æ ¸å¿ƒæœºåˆ¶** (app/scrapers/browser_use_scraper.py:215-284):
```python
async def scrape_with_task(self, task, output_model, max_steps=20, use_vision=True):
    # â‘  è·å–æˆ–åˆ›å»ºæµè§ˆå™¨ä¼šè¯ï¼ˆä½¿ç”¨å¢å¼ºåæ£€æµ‹çš„ browser_profileï¼‰
    browser_session = await self._get_browser_session()

    # â‘¡ åˆ›å»º Browser-Use AI Agent
    agent = Agent(
        task=task,                        # è‡ªç„¶è¯­è¨€ä»»åŠ¡
        llm=self.llm,                     # Google Gemini 2.0 Flash (é»˜è®¤)
        browser_session=browser_session,
        output_model_schema=output_model, # ç»“æ„åŒ–è¾“å‡º
        use_vision=use_vision             # ä½¿ç”¨è§†è§‰èƒ½åŠ›ç†è§£é¡µé¢
    )

    # â‘¢ AI è‡ªåŠ¨æ‰§è¡Œä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    # - æ‰“å¼€ç½‘ç«™
    # - è¾“å…¥æœç´¢å…³é”®è¯
    # - ç‚¹å‡»æœç´¢æŒ‰é’®
    # - æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
    # - ç‚¹å‡»è¿›å…¥ç¬”è®°è¯¦æƒ…é¡µ
    # - æå–æ•°æ®
    # - è¿”å›ä¸Šä¸€é¡µç»§ç»­çˆ¬å–
    history = await asyncio.wait_for(
        agent.run(max_steps=max_steps),
        timeout=settings.MAX_SCRAPE_TIMEOUT  # è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤300ç§’ï¼‰
    )

    # â‘£ è¿”å›ç»“æ„åŒ–ç»“æœ
    result = history.final_result()
    visited_urls = [item.state.url for item in history.history if hasattr(item.state, 'url')]

    # â‘¤ è¯¦ç»†è®°å½•æ¯ä¸€æ­¥çš„æ‰§è¡Œæƒ…å†µï¼ˆç”¨äºè°ƒè¯•å’Œåçˆ¬è™«åˆ†æï¼‰
    self._log_agent_steps(history)

    return {
        "status": "success",
        "data": result,                  # Pydantic æ¨¡å‹å¯¹è±¡
        "steps": len(history.history),   # æ‰§è¡Œæ­¥éª¤æ•°
        "urls": visited_urls             # è®¿é—®è¿‡çš„URLåˆ—è¡¨ï¼ˆç”¨äºéªŒè¯ç æ£€æµ‹ï¼‰
    }
```

**å¢å¼ºåæ£€æµ‹æµè§ˆå™¨é…ç½®** (app/scrapers/browser_use_scraper.py:75-123):
```python
def _create_browser_profile(self) -> BrowserProfile:
    """åˆ›å»ºæ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„æµè§ˆå™¨é…ç½®ï¼ˆå¢å¼ºåæ£€æµ‹ï¼‰"""

    # åŸºç¡€åæ£€æµ‹å‚æ•°
    browser_args = [
        '--disable-blink-features=AutomationControlled',  # éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
        '--disable-dev-shm-usage',
        '--disable-infobars',  # éšè—è‡ªåŠ¨åŒ–ä¿¡æ¯æ 
    ]

    # æ ¹æ®æœ‰å¤´/æ— å¤´æ¨¡å¼æ·»åŠ ä¸åŒå‚æ•°
    if self.headless:
        # æ— å¤´æ¨¡å¼ï¼šæ·»åŠ å¿…è¦å‚æ•°
        browser_args.extend([
            '--no-sandbox',
            '--disable-gpu',
            '--window-size=1920,1080',  # è®¾ç½®çª—å£å¤§å°
        ])
    else:
        # æœ‰å¤´æ¨¡å¼ï¼šæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
        browser_args.extend([
            '--start-maximized',  # æœ€å¤§åŒ–çª—å£ï¼ˆçœŸå®ç”¨æˆ·è¡Œä¸ºï¼‰
        ])

    # çœŸå®çš„ User-Agentï¼ˆMac Chromeï¼‰
    user_agent = (
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/120.0.0.0 Safari/537.36'
    )

    profile = BrowserProfile(
        headless=self.headless,
        disable_security=False,  # ä¿æŒå®‰å…¨ç‰¹æ€§,æ›´åƒçœŸå®æµè§ˆå™¨
        user_data_dir=None,
        args=browser_args,
        ignore_default_args=['--enable-automation'],  # éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
        wait_for_network_idle_page_load_time=2.0,  # å¢åŠ ç­‰å¾…,æ›´è‡ªç„¶
        maximum_wait_page_load_time=10.0,  # æ›´å……è¶³çš„åŠ è½½æ—¶é—´
        wait_between_actions=1.0,  # æ¨¡æ‹Ÿäººç±»æ“ä½œé€Ÿåº¦
    )

    return profile
```

**èµ„æºæ³„æ¼ä¿®å¤** (app/scrapers/browser_use_scraper.py:283-290):
```python
async def close(self):
    """å…³é—­æµè§ˆå™¨ä¼šè¯ï¼ˆä¿®å¤èµ„æºæ³„æ¼ï¼‰"""
    if self.browser_session:
        try:
            # Browser-Use 0.7.x ä½¿ç”¨ stop() æ–¹æ³•è€Œé close()
            if hasattr(self.browser_session, 'stop'):
                await self.browser_session.stop()
            elif hasattr(self.browser_session, 'close'):
                await self.browser_session.close()

            # ç­‰å¾…èµ„æºé‡Šæ”¾ï¼ˆä¿®å¤ aiohttp è¿æ¥æ³„æ¼ï¼‰
            await asyncio.sleep(0.1)

        except Exception as e:
            # é™çº§ä¸ºè­¦å‘Šï¼Œä¸å½±å“ä¸»æµç¨‹
            logger.warning(f"âš ï¸  å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
        finally:
            self.browser_session = None
```

**AI çš„ä¼˜åŠ¿**:
- âœ… è‡ªåŠ¨å¤„ç†é¡µé¢å˜åŒ–ï¼ˆä¸ä¾èµ– CSS é€‰æ‹©å™¨ï¼‰
- âœ… æ™ºèƒ½æ£€æµ‹å¹¶äººå·¥å¤„ç†éªŒè¯ç ï¼ˆè‡ªåŠ¨æš‚åœç­‰å¾…ï¼‰
- âœ… å¢å¼ºåæ£€æµ‹é…ç½®ï¼ˆéšè—è‡ªåŠ¨åŒ–æ ‡è¯†ã€çœŸå® User-Agentï¼‰
- âœ… æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼ˆé¼ æ ‡ç§»åŠ¨ã€æ»šåŠ¨ã€æ“ä½œé—´éš”ï¼‰
- âœ… è‡ªåŠ¨é‡è¯•å¤±è´¥æ“ä½œ
- âœ… èµ„æºæ³„æ¼ä¿®å¤ï¼ˆaiohttp è¿æ¥æ± ï¼‰

---

#### 6. æ•°æ®æµè½¬ç¤ºæ„å›¾

```
ç”¨æˆ·æµè§ˆå™¨ (http://localhost:8501)
         â”‚
         â”‚ HTTP/WebSocket
         â†“
Streamlit Server (åŒä¸€è¿›ç¨‹)
    frontend/app.py
         â”‚
         â”‚ asyncio.run()
         â†“
    PlannerAgent.plan_trip()
         â”‚
         â”‚ asyncio.gather() å¹¶å‘
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                      â†“                  â†“
    XHSScraper            XHSScraper         XHSScraper
    (æ™¯ç‚¹1:æ•…å®«)           (æ™¯ç‚¹2:é•¿åŸ)        (æ™¯ç‚¹3:é¢å’Œå›­)
         â”‚                      â”‚                  â”‚
         â”‚ Browser-Use AI       â”‚                  â”‚
         â†“                      â†“                  â†“
    å°çº¢ä¹¦ç½‘ç«™              å°çº¢ä¹¦ç½‘ç«™          å°çº¢ä¹¦ç½‘ç«™
    çˆ¬å–ç¬”è®°æ•°æ®            çˆ¬å–ç¬”è®°æ•°æ®        çˆ¬å–ç¬”è®°æ•°æ®
         â”‚                      â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“ æ”¶é›†ç»“æœ
                  TripPlan å¯¹è±¡
                        â”‚
                        â†“ æ ¼å¼åŒ–æ–‡æœ¬
                   Markdown æ–¹æ¡ˆ
                        â”‚
                        â†“ ä¿å­˜
              data/plans/åŒ—äº¬_7æ—¥æ¸¸.json
                        â”‚
                        â†“ æ˜¾ç¤º
              Streamlit UI (4ä¸ªTab)
```

---

#### 7. å®Œæ•´æ‰§è¡Œæ—¶é—´çº¿

å‡è®¾ç”¨æˆ·è¾“å…¥"åŒ—äº¬æ•…å®«ã€é•¿åŸã€é¢å’Œå›­"ï¼Œ3 ä¸ªæ™¯ç‚¹ï¼š

```
T=0s    ç”¨æˆ·ç‚¹å‡»"å¼€å§‹æ™ºèƒ½è§„åˆ’"
T=1s    åˆå§‹åŒ– PlannerAgent
T=2s    å¯åŠ¨ 3 ä¸ª AI çˆ¬è™«å®ä¾‹
T=3s    å¹¶å‘æ‰“å¼€ 3 ä¸ªæµè§ˆå™¨çª—å£
        â”œâ”€ æµè§ˆå™¨1: æ‰“å¼€å°çº¢ä¹¦ï¼Œæœç´¢"æ•…å®«"
        â”œâ”€ æµè§ˆå™¨2: æ‰“å¼€å°çº¢ä¹¦ï¼Œæœç´¢"é•¿åŸ"
        â””â”€ æµè§ˆå™¨3: æ‰“å¼€å°çº¢ä¹¦ï¼Œæœç´¢"é¢å’Œå›­"
T=5s    AI å¼€å§‹æ‰§è¡Œä»»åŠ¡
        â”œâ”€ æµè§ˆå™¨1: è¾“å…¥å…³é”®è¯ â†’ ç‚¹å‡»æœç´¢ â†’ æ»šåŠ¨é¡µé¢
        â”œâ”€ æµè§ˆå™¨2: è¾“å…¥å…³é”®è¯ â†’ ç‚¹å‡»æœç´¢ â†’ æ»šåŠ¨é¡µé¢
        â””â”€ æµè§ˆå™¨3: è¾“å…¥å…³é”®è¯ â†’ ç‚¹å‡»æœç´¢ â†’ æ»šåŠ¨é¡µé¢
T=15s   AI ç‚¹å‡»è¿›å…¥ç¬”è®°è¯¦æƒ…é¡µï¼Œæå–æ•°æ®
T=25s   3 ä¸ªçˆ¬è™«å…¨éƒ¨å®Œæˆï¼Œè¿”å›ç»“æœ
T=26s   å…³é—­æµè§ˆå™¨ä¼šè¯
T=27s   PlannerAgent ç”Ÿæˆæ—…è¡Œæ–¹æ¡ˆæ–‡æœ¬
T=28s   ä¿å­˜ JSON æ–‡ä»¶åˆ° data/plans/
T=29s   Streamlit å±•ç¤ºç»“æœ
```

**æ€»è€—æ—¶**: ~30 ç§’ï¼ˆä¼ ç»Ÿä¸²è¡Œéœ€è¦ 90 ç§’ï¼‰

---

#### 8. å…³é”®æŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ä½œç”¨ |
|------|------|------|
| **å‰ç«¯** | Streamlit | Web UI æ¡†æ¶ï¼Œå¤„ç†ç”¨æˆ·äº¤äº’ |
| **å¼‚æ­¥è°ƒåº¦** | asyncio | å¹¶å‘æ‰§è¡Œå¤šä¸ªçˆ¬è™«ä»»åŠ¡ |
| **AI å¼•æ“** | Browser-Use | AI æ§åˆ¶æµè§ˆå™¨è‡ªåŠ¨åŒ– |
| **æµè§ˆå™¨é©±åŠ¨** | Playwright | åº•å±‚æµè§ˆå™¨è‡ªåŠ¨åŒ–å¼•æ“ |
| **LLM** | Google Gemini 2.0 Flash | ç†è§£ä»»åŠ¡ã€å†³ç­–æ“ä½œ |
| **æ•°æ®éªŒè¯** | Pydantic | ç»“æ„åŒ–è¾“å‡ºå’Œç±»å‹éªŒè¯ |
| **æ—¥å¿—** | Python logging | è°ƒè¯•å’Œç›‘æ§ |

---

#### 9. è°ƒè¯•æŠ€å·§

**æŸ¥çœ‹æµè§ˆå™¨æ‰§è¡Œè¿‡ç¨‹**:
```bash
# æ–¹æ³•1: ä¿®æ”¹å‰ç«¯é…ç½®
# åœ¨ Streamlit ä¾§è¾¹æ å–æ¶ˆå‹¾é€‰"æ— å¤´æ¨¡å¼"

# æ–¹æ³•2: ç‹¬ç«‹è¿è¡Œçˆ¬è™«ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨ï¼‰
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" --no-headless
```

**æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**:
```python
# ä¿®æ”¹ config/settings.py
LOG_LEVEL = "DEBUG"  # æ˜¾ç¤ºæ›´è¯¦ç»†çš„æ—¥å¿—
```

**æŸ¥çœ‹ AI æ‰§è¡Œæ­¥éª¤**:
```python
# app/scrapers/browser_use_scraper.py:190
history = await agent.run(max_steps=30)
for step in history.history:
    print(f"Step {step.step_number}: {step.action}")
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
browser-brain/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ planner_agent.py      # æ—…è¡Œè§„åˆ’ Agentï¼ˆæ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼‰
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ browser_use_scraper.py # AI çˆ¬è™«åŸºç±»
â”‚   â”‚   â”œâ”€â”€ xhs_scraper.py         # å°çº¢ä¹¦çˆ¬è™«
â”‚   â”‚   â”œâ”€â”€ official_scraper.py    # å®˜ç½‘çˆ¬è™«
â”‚   â”‚   â”œâ”€â”€ run_xhs.py            # å°çº¢ä¹¦ç‹¬ç«‹è¿è¡Œè„šæœ¬
â”‚   â”‚   â””â”€â”€ run_official.py       # å®˜ç½‘ç‹¬ç«‹è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ attraction.py          # æ™¯ç‚¹æ•°æ®æ¨¡å‹
â”‚   â”‚   â””â”€â”€ trip_plan.py           # æ—…è¡Œæ–¹æ¡ˆæ¨¡å‹
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py              # æ—¥å¿—å·¥å…·
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                     # Streamlit Web ç•Œé¢
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                # é…ç½®ç®¡ç†
â”œâ”€â”€ run_xhs_scraper.sh            # å°çº¢ä¹¦æ”¶é›†å™¨å¯åŠ¨è„šæœ¬
â”œâ”€â”€ run_official_scraper.sh       # å®˜ç½‘æ”¶é›†å™¨å¯åŠ¨è„šæœ¬
â””â”€â”€ .pylintrc                      # Pylint é…ç½®
```

### å‰åç«¯é€šä¿¡æ¶æ„

**Streamlit å•ä½“æ¶æ„**ï¼šå‰åç«¯è¿è¡Œåœ¨åŒä¸€è¿›ç¨‹ï¼Œé€šè¿‡ç›´æ¥å‡½æ•°è°ƒç”¨é€šä¿¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        æµè§ˆå™¨ (Browser)              â”‚
â”‚      http://localhost:8501          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP/WebSocket
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Streamlit Server (åŒä¸€è¿›ç¨‹)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Frontend (frontend/app.py)    â”‚ â”‚
â”‚  â”‚  - UI ç»„ä»¶                      â”‚ â”‚
â”‚  â”‚  - Session State               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â”‚ ç›´æ¥å‡½æ•°è°ƒç”¨            â”‚
â”‚             â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Backend Logic (app/)          â”‚ â”‚
â”‚  â”‚  - PlannerAgent                â”‚ â”‚
â”‚  â”‚  - Scrapers                    â”‚ â”‚
â”‚  â”‚  - Models                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ•°æ®æµ**ï¼š
```
ç”¨æˆ·è¾“å…¥ â†’ Streamlit UI â†’ PlannerAgent
                              â†“
                    å¹¶å‘å¯åŠ¨ AI çˆ¬è™«
                    â”œâ”€â”€ XHSScraperï¼ˆå°çº¢ä¹¦ï¼‰
                    â””â”€â”€ OfficialScraperï¼ˆå®˜ç½‘ï¼‰
                              â†“
                    æ”¶é›†æ™¯ç‚¹ä¿¡æ¯ â†’ Attraction å¯¹è±¡
                              â†“
                    ç”Ÿæˆè¡Œç¨‹æ–¹æ¡ˆ â†’ TripPlan å¯¹è±¡
                              â†“
                    æ ¼å¼åŒ–è¾“å‡º â†’ æ˜¾ç¤ºç»™ç”¨æˆ·
```

**å…³é”®ä»£ç ** (frontend/app.py:283-346):
```python
async def run_planning():
    # å‰ç«¯ç›´æ¥åˆ›å»ºåç«¯å¯¹è±¡ï¼ˆåŒè¿›ç¨‹ï¼‰
    planner = PlannerAgent(
        headless=headless_mode,
        log_callback=add_log  # å°†æ—¥å¿—å®æ—¶ä¼ é€’ç»™å‰ç«¯
    )

    try:
        # ç›´æ¥è°ƒç”¨åç«¯æ–¹æ³•
        result = await planner.plan_trip(
            departure=departure,
            destination=destination,
            days=int(days),
            must_visit=must_visit_list
        )

        # ä¿å­˜æ–¹æ¡ˆåˆ° JSON æ–‡ä»¶
        plan_data = {
            "timestamp": datetime.now().isoformat(),
            "departure": departure,
            "destination": destination,
            "days": days,
            "must_visit": must_visit_list,
            "plan_text": result,
            "logs": st.session_state.planning_logs
        }

        return result, None

    except Exception as e:
        return None, str(e)

# ä½¿ç”¨ asyncio.run æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ï¼ˆå‰åç«¯åŒä¸€è¿›ç¨‹ï¼‰
result, error = asyncio.run(run_planning())
```

## ğŸ”‘ å…³é”®è®¾è®¡æ¨¡å¼

### 1. ç‹¬ç«‹æ”¶é›†å™¨æ¨¡å¼ï¼ˆæ–°å¢ï¼‰

æ¯ä¸ªæ”¶é›†å™¨å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œ JSON è¾“å‡ºã€‚

#### å°çº¢ä¹¦æ”¶é›†å™¨

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
# åŸºç¡€ç”¨æ³•
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 5

# Python ç›´æ¥è¿è¡Œ
.venv/bin/python app/scrapers/run_xhs.py "åŒ—äº¬æ•…å®«" --max-notes 5

# è°ƒè¯•æ¨¡å¼ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨ï¼‰
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" --no-headless
```

**è¾“å‡ºæ ¼å¼** (app/scrapers/run_xhs.py:56-79):
```json
{
  "attraction": "åŒ—äº¬æ•…å®«",
  "total_notes": 5,
  "notes": [
    {
      "note_id": "xhs_åŒ—äº¬æ•…å®«_0",
      "title": "æ•…å®«æ¸¸ç©æ”»ç•¥...",
      "author": "æ—…è¡Œè¾¾äºº",
      "content": "...",
      "likes": 1250,
      "collects": 980,
      "images_count": 9,
      "created_at": "2025-10-05T18:36:22"
    }
  ]
}
```

#### å®˜ç½‘æ”¶é›†å™¨

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
# åŸºç¡€ç”¨æ³•
./run_official_scraper.sh "åŒ—äº¬æ•…å®«"

# å¸¦å‚è€ƒé“¾æ¥
./run_official_scraper.sh "åŒ—äº¬æ•…å®«" -l "https://www.dpm.org.cn"

# Python ç›´æ¥è¿è¡Œ
.venv/bin/python app/scrapers/run_official.py "åŒ—äº¬æ•…å®«" -l "https://www.dpm.org.cn"
```

**è¾“å‡ºæ ¼å¼** (app/scrapers/run_official.py:51-71):
```json
{
  "attraction": "åŒ—äº¬æ•…å®«",
  "official_info": {
    "website": "https://www.dpm.org.cn",
    "opening_hours": "8:30-17:00ï¼ˆå‘¨ä¸€é—­é¦†ï¼‰",
    "ticket_price": "æˆäººç¥¨60å…ƒï¼Œå­¦ç”Ÿç¥¨20å…ƒ",
    "booking_method": "å®˜ç½‘å®åé¢„çº¦",
    "address": "åŒ—äº¬å¸‚ä¸œåŸåŒºæ™¯å±±å‰è¡—4å·",
    "phone": "010-85007421",
    "description": "..."
  }
}
```

#### ç»„åˆä½¿ç”¨ç¤ºä¾‹

```bash
# 1. æ”¶é›†å°çº¢ä¹¦ç¬”è®°
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 5 > xhs_result.json

# 2. æå–é“¾æ¥å¹¶ä¼ é€’ç»™å®˜ç½‘æ”¶é›†å™¨
./run_official_scraper.sh "åŒ—äº¬æ•…å®«" -l \
  "https://www.dpm.org.cn" \
  "https://gugong.228.com.cn" > official_result.json
```

### 2. ä¸Šä¸‹æ–‡æ•°æ®æ¨¡å‹ï¼ˆContext-Based Designï¼‰

**é‡è¦**: `TripPlan` å’Œ `Attraction` ä½¿ç”¨çµæ´»çš„ä¸Šä¸‹æ–‡å­—å…¸è€Œéå›ºå®šå±æ€§ã€‚

```python
# âŒ é”™è¯¯ï¼šç›´æ¥è®¿é—®å±æ€§
plan.daily_itineraries  # AttributeError!

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ get() æ–¹æ³•
itinerary_data = plan.get("ai_planning.itinerary", {})
highlights = plan.get("ai_planning.highlights", [])
budget = plan.get("ai_planning.budget", {})
```

**TripPlan çš„ context ç»“æ„**:
```python
{
    "collected_data": {
        "attractions": [],      # Attraction å¯¹è±¡åˆ—è¡¨
        "hotels": [],
        "transports": [],
        "other_info": {}
    },
    "ai_planning": {           # AI è‡ªç”±å¡«å……
        "itinerary": {
            "day1": {...},
            "day2": {...}
        },
        "highlights": [],
        "tips": [],
        "budget": {}
    },
    "user_preferences": {},
    "metadata": {}
}
```

### 3. Browser-Use AI é›†æˆä¸é€Ÿåº¦ä¼˜åŒ–

æ‰€æœ‰çˆ¬è™«ç»§æ‰¿è‡ª `BrowserUseScraper` åŸºç±» (app/scrapers/browser_use_scraper.py:30-340)ï¼š

```python
class XHSScraper(BrowserUseScraper):
    async def scrape(self, attraction_name: str, max_notes: int):
        # AI ä»»åŠ¡æè¿°
        task = f"""
        åœ¨å°çº¢ä¹¦æœç´¢'{attraction_name}'ï¼Œ
        æ”¶é›†æœ€çƒ­é—¨çš„{max_notes}æ¡ç¬”è®°...
        """

        # AI æ‰§è¡Œä»»åŠ¡ï¼ˆä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼‰
        result = await self.scrape_with_task(
            task=task,
            output_model=XHSNotesCollection,  # Pydantic æ¨¡å‹
            max_steps=30
        )
        return self._parse_result(result)
```

**Browser-Use æ ¸å¿ƒæ–¹æ³•** (app/scrapers/browser_use_scraper.py:215-300):
```python
async def scrape_with_task(
    self,
    task: str,
    output_model: Optional[type[BaseModel]] = None,
    max_steps: int = 20,
    use_vision: bool = True
) -> dict:
    """
    ä½¿ç”¨Browser-Use Agentæ‰§è¡Œçˆ¬å–ä»»åŠ¡

    Args:
        task: ä»»åŠ¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
        output_model: Pydanticæ¨¡å‹ç±»ï¼Œç”¨äºç»“æ„åŒ–è¾“å‡º
        max_steps: æœ€å¤§æ­¥éª¤æ•°
        use_vision: æ˜¯å¦ä½¿ç”¨è§†è§‰èƒ½åŠ›ï¼ˆæˆªå›¾ç†è§£ï¼‰
    """
    agent = Agent(
        task=task,
        llm=self.llm,
        browser_session=browser_session,
        output_model_schema=output_model,
        use_vision=use_vision
    )

    history = await agent.run(max_steps=max_steps)
    result = history.final_result()

    return {
        "status": "success",
        "data": result,
        "steps": len(history.history),
        "urls": visited_urls
    }
```

### 4. Pydantic æ•°æ®éªŒè¯

**é‡è¦**: æ‰€æœ‰æ¨¡å‹å­—æ®µå¿…é¡»ç±»å‹åŒ¹é…ã€‚

å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆï¼š
```python
# âŒ é”™è¯¯1ï¼šç±»å‹ä¸åŒ¹é…
DailyItinerary(
    date=datetime.date(2025, 10, 4),  # æœŸæœ› str
    notes=["æç¤º1", "æç¤º2"]           # æœŸæœ› str
)

# âœ… æ­£ç¡®ï¼šç±»å‹è½¬æ¢
DailyItinerary(
    date=str(datetime.date(2025, 10, 4)),
    notes="\n".join(["æç¤º1", "æç¤º2"])
)

# âŒ é”™è¯¯2ï¼šdatetime å¯¹è±¡ç›´æ¥èµ‹å€¼
XHSNote(
    created_at=datetime.now()  # æœŸæœ› str
)

# âœ… æ­£ç¡®ï¼šè½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
XHSNote(
    created_at=datetime.now().isoformat()
)
```

**XHSNote æ¨¡å‹å®šä¹‰** (app/models/attraction.py:46-57):
```python
class XHSNote(BaseModel):
    """å°çº¢ä¹¦ç¬”è®°æ•°æ®æ¨¡å‹"""
    note_id: str = Field(default="", description="ç¬”è®°å”¯ä¸€ID")
    title: str = Field(description="ç¬”è®°æ ‡é¢˜")
    author: str = Field(default="", description="ä½œè€…åç§°")
    content: str = Field(default="", description="ç¬”è®°æ­£æ–‡å†…å®¹")
    likes: int = Field(default=0, description="ç‚¹èµæ•°")
    collects: int = Field(default=0, description="æ”¶è—æ•°")
    comments: int = Field(default=0, description="è¯„è®ºæ•°")
    images: List[str] = Field(default_factory=list, description="å›¾ç‰‡URLåˆ—è¡¨")
    url: str = Field(default="", description="ç¬”è®°é“¾æ¥")
    extracted_links: List[str] = Field(default_factory=list, description="æå–çš„URLé“¾æ¥")
    keywords: List[str] = Field(default_factory=list, description="å…³é”®è¯")
    created_at: Optional[datetime] = Field(default=None, description="å‘å¸ƒæ—¶é—´")
```

### 5. å¼‚æ­¥å¹¶å‘

ä½¿ç”¨ `asyncio.gather()` å®ç°å¹¶å‘çˆ¬å– (app/agents/planner_agent.py:120-150)ï¼š

```python
# å¹¶å‘çˆ¬å–å¤šä¸ªæ™¯ç‚¹
tasks = [
    self._scrape_single_attraction_ai(dest, attr, xhs, official)
    for attr in must_visit
]
results = await asyncio.gather(*tasks, return_exceptions=True)

# é”™è¯¯å¤„ç†
for idx, result in enumerate(results):
    if isinstance(result, Exception):
        logger.error(f"æ™¯ç‚¹ {must_visit[idx]} çˆ¬å–å¤±è´¥: {result}")
    else:
        self.attractions.append(result)
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡ï¼ˆ.envï¼‰

```bash
# LLM é…ç½®
LLM_PROVIDER=google             # openai/anthropic/google
LLM_MODEL=gemini-2.0-flash-exp  # æ¨¡å‹åç§°
GOOGLE_API_KEY=AIza...          # API å¯†é’¥

# çˆ¬è™«é…ç½®
HEADLESS=true                   # æ— å¤´æµè§ˆå™¨æ¨¡å¼
XHS_MAX_NOTES=5                 # å°çº¢ä¹¦ç¬”è®°æ•°é‡
MAX_SCRAPE_TIMEOUT=300          # çˆ¬å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# Web ç•Œé¢
STREAMLIT_SERVER_PORT=8501
```

### Pylint é…ç½®ï¼ˆ.pylintrcï¼‰

- ç¦ç”¨è¿‡äºä¸¥æ ¼çš„æ£€æŸ¥ï¼ˆC0103, C0114, C0115, C0116, R0903, R0913ï¼‰
- å…è®¸çŸ­å˜é‡åï¼š`i, j, k, ex, _, id, db, st`
- æœ€å¤§è¡Œé•¿åº¦ï¼š120
- ç™½åå•æ‰©å±•åŒ…ï¼š`pydantic`

## ğŸ› å¸¸è§é—®é¢˜

### 1. AIçˆ¬è™«å¤±è´¥ï¼šLLMå…¼å®¹æ€§é—®é¢˜

**ç—‡çŠ¶**: `ValueError: "ChatGoogleGenerativeAI" object has no field "ainvoke"`

**åŸå› **: browser-use 0.7.x ä¸ `langchain-google-genai.ChatGoogleGenerativeAI` ä¸å…¼å®¹

**è§£å†³**: å·²ä¿®å¤ï¼Œä½¿ç”¨browser-useåŸç”Ÿæ”¯æŒçš„ ChatGoogle (app/scrapers/browser_use_scraper.py:38-72)
```python
# ä½¿ç”¨ browser-use å†…ç½®çš„ ChatGoogle
if provider == "google":
    from browser_use import ChatGoogle
    return ChatGoogle(
        model=model,
        api_key=settings.GOOGLE_API_KEY
    )
```

**å‚è€ƒ**: `tests/bug_report_20251005.md`

### 2. Python ç¼“å­˜å¯¼è‡´ä»£ç ä¸ç”Ÿæ•ˆ

**ç—‡çŠ¶**: ä¿®æ”¹ä»£ç åè¿è¡Œä»ä½¿ç”¨æ—§é€»è¾‘

**è§£å†³**:
```bash
# æ¸…ç†æ‰€æœ‰ Python ç¼“å­˜
find app -name "*.pyc" -delete
find app -name "__pycache__" -type d -exec rm -rf {} +

# é‡å¯ Streamlit æœåŠ¡
pkill -f streamlit
./run_web.sh
```

### 3. æ¨¡å‹å­—æ®µç±»å‹é”™è¯¯

**ç—‡çŠ¶**: `ValidationError: Input should be a valid string [type=string_type]`

**åŸå› **: Pydantic ä¸¥æ ¼ç±»å‹æ£€æŸ¥ï¼Œdatetime å¯¹è±¡ä¸èƒ½ç›´æ¥èµ‹å€¼ç»™ str å­—æ®µ

**è§£å†³**: ä½¿ç”¨ `.isoformat()` è½¬æ¢
```python
# âŒ é”™è¯¯
created_at=datetime.now()

# âœ… æ­£ç¡®
created_at=datetime.now().isoformat()
```

### 4. AI è¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸

**ç—‡çŠ¶**: `AttributeError: 'str' object has no attribute 'website'`

**åŸå› **: AI æœ‰æ—¶è¿”å›å­—ç¬¦ä¸²è€Œéç»“æ„åŒ–å¯¹è±¡

**è§£å†³**: å·²åœ¨ä»£ç ä¸­å¤„ç† (app/scrapers/official_scraper.py:140-143)
```python
# å¤„ç† AI Agent å¤±è´¥è¿”å› None æˆ–å­—ç¬¦ä¸²çš„æƒ…å†µ
if data is None or isinstance(data, str):
    logger.warning(f"âš ï¸  AIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {type(data)}")
    return None
```

### 5. Pylint å¯¼å…¥é”™è¯¯ï¼ˆE0401/E0611ï¼‰

**ç—‡çŠ¶**: `Unable to import 'app.models.attraction'`

**åŸå› **: Pylint é™æ€åˆ†æé™åˆ¶ï¼Œè¿è¡Œæ—¶æ­£å¸¸

**è§£å†³**: å·²åœ¨ `.pylintrc` é…ç½®ï¼Œå¯å¿½ç•¥

### 6. TripPlan AttributeError

**ç—‡çŠ¶**: `'TripPlan' object has no attribute 'daily_itineraries'`

**åŸå› **: ä½¿ç”¨äº†ç›´æ¥å±æ€§è®¿é—®è€Œé `get()` æ–¹æ³•

**è§£å†³**: å‚è€ƒ"ä¸Šä¸‹æ–‡æ•°æ®æ¨¡å‹"ç« èŠ‚

### 7. Streamlit ç«¯å£å ç”¨

**ç—‡çŠ¶**: `Address already in use`

**è§£å†³**:
```bash
# æ€æ­»å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -ti:8501 | xargs kill -9

# é‡æ–°å¯åŠ¨
./run_web.sh
```

### 8. å°çº¢ä¹¦åçˆ¬è™«é™åˆ¶

**ç—‡çŠ¶**: AI æŠ¥å‘Š "security restriction error" æˆ– "éªŒè¯ç "

**åŸå› **: å°çº¢ä¹¦æ£€æµ‹åˆ°è‡ªåŠ¨åŒ–è¡Œä¸º

**è§£å†³**: å·²å®ç°è‡ªåŠ¨åŒ–éªŒè¯ç å¤„ç†æœºåˆ¶ (app/scrapers/xhs_scraper.py:34-51, 87-104)

```python
# éªŒè¯ç äººå·¥å¤„ç†æœºåˆ¶
async def _handle_captcha_manual(self, wait_seconds: int = 60):
    """
    éªŒè¯ç äººå·¥å¤„ç†ï¼šæš‚åœç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯

    Args:
        wait_seconds: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’
    """
    logger.warning("âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæš‚åœç­‰å¾…äººå·¥å¤„ç†...")
    logger.info("ğŸ“Œ è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®ŒæˆéªŒè¯ç éªŒè¯")
    logger.info(f"â³ ç³»ç»Ÿå°†åœ¨ {wait_seconds} ç§’åè‡ªåŠ¨ç»§ç»­...")

    # æ¯10ç§’æç¤ºä¸€æ¬¡å‰©ä½™æ—¶é—´
    for remaining in range(wait_seconds, 0, -10):
        logger.info(f"â±ï¸  å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining} ç§’")
        await asyncio.sleep(min(10, remaining))

# è‡ªåŠ¨æ£€æµ‹éªŒè¯ç å¹¶è§¦å‘äººå·¥å¤„ç†
if any("captcha" in url.lower() for url in visited_urls):
    logger.warning("ğŸš« æ£€æµ‹åˆ°è®¿é—®äº†éªŒè¯ç é¡µé¢ï¼Œå¯åŠ¨äººå·¥å¤„ç†æµç¨‹...")
    await self._handle_captcha_manual(wait_seconds=60)
    # é‡æ–°å°è¯•æ‰§è¡Œä»»åŠ¡
    result = await self.scrape_with_task(task=task, output_model=XHSNotesCollection, max_steps=30)
```

**å…¶ä»–ä¼˜åŒ–æªæ–½**:
- âœ… ä½¿ç”¨å¢å¼ºåæ£€æµ‹æµè§ˆå™¨é…ç½®ï¼ˆéšè—è‡ªåŠ¨åŒ–æ ‡è¯†ã€çœŸå® User-Agentï¼‰
- âœ… é»˜è®¤ä½¿ç”¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼ï¼ˆ`HEADLESS=false`ï¼‰
- âœ… æ¨¡æ‹Ÿäººç±»æ“ä½œé€Ÿåº¦ï¼ˆ`wait_between_actions=1.0`ï¼‰
- é™ä½çˆ¬å–é¢‘ç‡ï¼ˆå‡å°‘ `max_notes` å‚æ•°ï¼‰
- æ›´æ¢ IP åœ°å€
- ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•

## ğŸ”¬ æ ¸å¿ƒå¼€å‘åŸåˆ™

### âš ï¸ é—®é¢˜è°ƒè¯•çš„æ­£ç¡®æ–¹æ³•

**é‡è¦åŸåˆ™**: é‡åˆ°é—®é¢˜ä¸ä¸€è‡´æ—¶ï¼Œ**å¿…é¡»æ·±å…¥åˆ†æé—®é¢˜æ ¹æº**ï¼Œè€Œä¸æ˜¯ç»•è¿‡é—®é¢˜ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚

#### æ­£ç¡®çš„é—®é¢˜å¤„ç†æµç¨‹

```
1. ğŸ” å‘ç°é—®é¢˜
   â†“
2. ğŸ“Š æ”¶é›†ç°è±¡å’Œæ—¥å¿—
   â†“
3. ğŸ§  åˆ†æé—®é¢˜æ ¹æœ¬åŸå› 
   â†“
4. ğŸ”¬ é€å±‚æ·±å…¥éªŒè¯å‡è®¾
   â†“
5. âœ… æ‰¾åˆ°çœŸæ­£çš„é—®é¢˜æ‰€åœ¨
   â†“
6. ğŸ”§ ä¿®å¤æ ¹æœ¬é—®é¢˜
   â†“
7. âœ“ éªŒè¯ä¿®å¤æ•ˆæœ
```

#### âŒ é”™è¯¯çš„å¤„ç†æ–¹å¼

```python
# ç¤ºä¾‹ï¼šAI çˆ¬è™«è¿”å›ç©ºæ•°æ®

# âŒ é”™è¯¯åšæ³•ï¼šç»•è¿‡é—®é¢˜
if result is None:
    # ç›´æ¥è¿”å›ç©ºæ•°æ®ï¼Œæˆ–è€…æ¢ä¸ªæ–¹æ³•
    return []

# âŒ é”™è¯¯åšæ³•ï¼šéšä¾¿æ”¹å…¶ä»–æ–¹æ³•è¾¾æˆç›®çš„
# æ¯”å¦‚ AI çˆ¬è™«å¤±è´¥ï¼Œå°±æ¢æˆä¼ ç»Ÿçˆ¬è™«
# è¿™æ ·ä¼šå¯¼è‡´æ¶æ„æ··ä¹±ï¼Œé—®é¢˜ç´¯ç§¯
```

#### âœ… æ­£ç¡®çš„å¤„ç†æ–¹å¼

```python
# ç¤ºä¾‹ï¼šAI çˆ¬è™«è¿”å›ç©ºæ•°æ®

# âœ… æ­£ç¡®åšæ³•ï¼šåˆ†æé—®é¢˜æ ¹æº

# æ­¥éª¤1: æ£€æŸ¥æ—¥å¿—ï¼Œæ‰¾åˆ°å¤±è´¥åŸå› 
logger.info("æ£€æŸ¥ Browser-Use Agent æ‰§è¡Œæ­¥éª¤")
self._log_agent_steps(history)

# æ­¥éª¤2: åˆ†æ AI çš„æ¯ä¸€æ­¥æ“ä½œ
# - AI æ˜¯å¦æˆåŠŸæ‰“å¼€ç½‘ç«™ï¼Ÿ
# - AI æ˜¯å¦é‡åˆ°åçˆ¬è™«æ‹¦æˆªï¼Ÿ
# - AI æ˜¯å¦æ­£ç¡®è¯†åˆ«é¡µé¢å…ƒç´ ï¼Ÿ
# - AI è¿”å›çš„æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Ÿ

# æ­¥éª¤3: æ ¹æ®åˆ†æç»“æœï¼Œé’ˆå¯¹æ€§ä¿®å¤
if "security restriction" in evaluation:
    logger.error("æ£€æµ‹åˆ°åçˆ¬è™«ï¼Œéœ€è¦ä¼˜åŒ–æµè§ˆå™¨é…ç½®")
    # ä¿®æ”¹ browser_profileï¼Œæ·»åŠ æ›´çœŸå®çš„æ¨¡æ‹Ÿ
elif result_data is None:
    logger.error("AI æœªè¿”å›æ•°æ®ï¼Œæ£€æŸ¥ output_model å®šä¹‰")
    # æ£€æŸ¥ Pydantic æ¨¡å‹æ˜¯å¦æ­£ç¡®

# æ­¥éª¤4: ä¿®å¤åéªŒè¯
# é‡æ–°è¿è¡Œï¼Œç¡®è®¤é—®é¢˜è§£å†³
```

#### å®é™…æ¡ˆä¾‹

**æ¡ˆä¾‹1: Browser-Use LLM å…¼å®¹æ€§é—®é¢˜**

```python
# âŒ é”™è¯¯åšæ³•ï¼šå‘ç° langchain-google-genai ä¸å…¼å®¹ï¼Œå°±æ¢æˆ OpenAI
# è¿™æ ·ä¼šå¯¼è‡´ä¾èµ– Google API Key çš„ç”¨æˆ·æ— æ³•ä½¿ç”¨

# âœ… æ­£ç¡®åšæ³•ï¼šæ·±å…¥åˆ†æ
# 1. æ£€æŸ¥ browser-use ç‰ˆæœ¬å’Œæ–‡æ¡£
# 2. å‘ç° browser-use 0.7.x å†…ç½®äº† ChatGoogle
# 3. é˜…è¯»æºç ï¼Œæ‰¾åˆ°æ­£ç¡®çš„å¯¼å…¥æ–¹å¼
from browser_use import ChatGoogle  # ä½¿ç”¨å†…ç½®çš„ç±»

# 4. ä¿®å¤é—®é¢˜
return ChatGoogle(model=model, api_key=settings.GOOGLE_API_KEY)
```

**æ¡ˆä¾‹2: å°çº¢ä¹¦åçˆ¬è™«é—®é¢˜**

```python
# âŒ é”™è¯¯åšæ³•ï¼šå°çº¢ä¹¦çˆ¬è™«è¢«æ‹¦æˆªï¼Œå°±æ¢æˆçˆ¬å–ç™¾åº¦ç™¾ç§‘
# è¿™æ ·ä¼šå¯¼è‡´æ•°æ®è´¨é‡ä¸‹é™

# âœ… æ­£ç¡®åšæ³•ï¼šåˆ†æåçˆ¬è™«æœºåˆ¶
# 1. æ£€æŸ¥ AI æ‰§è¡Œæ—¥å¿—ï¼Œæ‰¾åˆ°è¢«æ‹¦æˆªçš„æ­¥éª¤
# 2. åˆ†æåçˆ¬è™«æ£€æµ‹ç‚¹ï¼ˆUser-Agentã€è‡ªåŠ¨åŒ–æ ‡è¯†ã€è¡Œä¸ºç‰¹å¾ï¼‰
# 3. ä¼˜åŒ–æµè§ˆå™¨é…ç½®
browser_args = [
    '--disable-blink-features=AutomationControlled',  # éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
]
profile = BrowserProfile(
    disable_security=False,  # ä¿æŒå®‰å…¨ç‰¹æ€§ï¼Œæ›´åƒçœŸå®æµè§ˆå™¨
    wait_between_actions=1.0,  # æ¨¡æ‹Ÿäººç±»æ“ä½œé€Ÿåº¦
)

# 4. éªŒè¯ä¿®å¤æ•ˆæœï¼ˆä½¿ç”¨æœ‰å¤´æµè§ˆå™¨è§‚å¯Ÿï¼‰
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 2  # è§‚å¯Ÿ AI æ“ä½œè¿‡ç¨‹
```

**æ¡ˆä¾‹3: Pydantic æ•°æ®éªŒè¯å¤±è´¥**

```python
# âŒ é”™è¯¯åšæ³•ï¼šæ•°æ®éªŒè¯å¤±è´¥ï¼Œå°±æŠŠå­—æ®µæ”¹æˆ Optional æˆ–å»æ‰éªŒè¯
class XHSNote(BaseModel):
    created_at: Any  # é”™è¯¯ï¼šå¤±å»ç±»å‹å®‰å…¨

# âœ… æ­£ç¡®åšæ³•ï¼šåˆ†ææ•°æ®ç±»å‹ä¸åŒ¹é…çš„åŸå› 
# 1. æ£€æŸ¥ AI è¿”å›çš„åŸå§‹æ•°æ®
logger.debug(f"AI è¿”å›æ•°æ®: {result}")

# 2. å‘ç° created_at æ˜¯ datetime å¯¹è±¡ï¼Œä½†å­—æ®µå®šä¹‰æ˜¯ str
# 3. ä¿®æ­£æ•°æ®è½¬æ¢é€»è¾‘
note = XHSNote(
    created_at=datetime.now().isoformat()  # è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²
)

# æˆ–è€…ä¿®æ”¹æ¨¡å‹å®šä¹‰
class XHSNote(BaseModel):
    created_at: Optional[datetime] = None  # ç›´æ¥æ”¯æŒ datetime ç±»å‹
```

#### è°ƒè¯•å·¥å…·å’ŒæŠ€å·§

**1. ä½¿ç”¨è¯¦ç»†æ—¥å¿—**
```bash
# å¼€å¯ DEBUG çº§åˆ«æ—¥å¿—
export LOG_LEVEL=DEBUG
./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 2
```

**2. ä½¿ç”¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼**
```bash
# è§‚å¯Ÿ AI å®é™…æ“ä½œè¿‡ç¨‹ï¼ˆé»˜è®¤é…ç½®ï¼‰
HEADLESS=false ./run_xhs_scraper.sh "åŒ—äº¬æ•…å®«" -n 2
```

**3. æŸ¥çœ‹ Browser-Use Agent æ‰§è¡Œæ­¥éª¤**
```python
# åœ¨ browser_use_scraper.py ä¸­å·²å®ç°
self._log_agent_steps(history)

# æ—¥å¿—è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸ“ Step 1/10:
#    âš–ï¸  è¯„ä¼°: é¡µé¢åŠ è½½æˆåŠŸ
#    ğŸ¯ ä¸‹ä¸€æ­¥ç›®æ ‡: è¾“å…¥æœç´¢å…³é”®è¯
#    ğŸ¦¾ æ‰§è¡ŒåŠ¨ä½œ: type_text
#    ğŸ”— å½“å‰é¡µé¢: https://www.xiaohongshu.com
```

**4. åˆ†æ API è°ƒç”¨**
```python
# ä½¿ç”¨æ—¥å¿—å·¥å…·è®°å½• API è¯·æ±‚å’Œå“åº”
log_api_call("Browser-Use Agent", request_data={"task": task})
log_api_call("Browser-Use Agent", response_data=result, status="success")
```

#### é—®é¢˜æ’æŸ¥æ¸…å•

å½“é‡åˆ°é—®é¢˜æ—¶ï¼ŒæŒ‰ä»¥ä¸‹æ¸…å•é€é¡¹æ£€æŸ¥ï¼š

- [ ] **æ—¥å¿—è¾“å‡º**: æ˜¯å¦æœ‰ ERROR æˆ– WARNING æ—¥å¿—ï¼Ÿ
- [ ] **æ‰§è¡Œæ­¥éª¤**: AI Agent æ‰§è¡Œäº†å“ªäº›æ­¥éª¤ï¼Ÿåœ¨å“ªä¸€æ­¥å¤±è´¥ï¼Ÿ
- [ ] **ç½‘ç»œè¯·æ±‚**: API è°ƒç”¨æ˜¯å¦æˆåŠŸï¼Ÿè¿”å›äº†ä»€ä¹ˆæ•°æ®ï¼Ÿ
- [ ] **æ•°æ®æ ¼å¼**: è¿”å›çš„æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆ Pydantic æ¨¡å‹å®šä¹‰ï¼Ÿ
- [ ] **æµè§ˆå™¨çŠ¶æ€**: ä½¿ç”¨æœ‰å¤´æ¨¡å¼è§‚å¯Ÿæµè§ˆå™¨ï¼Œæ˜¯å¦æœ‰å¼¹çª—ã€éªŒè¯ç ã€æ‹¦æˆªï¼Ÿ
- [ ] **ç¯å¢ƒé…ç½®**: `.env` æ–‡ä»¶æ˜¯å¦æ­£ç¡®ï¼ŸAPI Key æ˜¯å¦æœ‰æ•ˆï¼Ÿ
- [ ] **ä¾èµ–ç‰ˆæœ¬**: ä¾èµ–åŒ…ç‰ˆæœ¬æ˜¯å¦å…¼å®¹ï¼Ÿæ˜¯å¦æœ‰ breaking changesï¼Ÿ
- [ ] **ä»£ç é€»è¾‘**: æ˜¯å¦æœ‰ if/else åˆ†æ”¯æœªè¦†ç›–çš„æƒ…å†µï¼Ÿ

#### æ€»ç»“

**ç‰¢è®°è¿™æ¡åŸåˆ™**ï¼š
> ğŸ¯ **ä¸è¦ç»•è¿‡é—®é¢˜ï¼Œè¦è§£å†³é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚**
>
> **ä¸´æ—¶æ–¹æ¡ˆä¼šç§¯ç´¯æŠ€æœ¯å€ºåŠ¡ï¼Œåªæœ‰å½»åº•è§£å†³é—®é¢˜æ‰èƒ½ä¿è¯ç³»ç»Ÿç¨³å®šæ€§ã€‚**

---

## âš¡ Fast Mode é€Ÿåº¦ä¼˜åŒ–ç­–ç•¥

### æŠ€æœ¯èƒŒæ™¯

åŸºäº Browser-Use å®˜æ–¹ Fast Agent æ¨¡æ¿ä¼˜åŒ–çˆ¬è™«æ€§èƒ½ï¼Œæ˜¾è‘—æå‡æ‰§è¡Œé€Ÿåº¦ã€‚

**ä¼˜åŒ–æ¥æº**: Browser-Use Fast Agent Template (2025)

### æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

#### 1. **Flash Modeï¼ˆLLM ä¼˜åŒ–ï¼‰**

**åŸç†**: ç¦ç”¨ LLM çš„"thinking"è¿‡ç¨‹ï¼Œç›´æ¥è¾“å‡ºå†³ç­–

```python
agent = Agent(
    task=task,
    llm=llm,
    flash_mode=True,  # å…³é”®ä¼˜åŒ–ï¼šè·³è¿‡æ€è€ƒç›´æ¥æ‰§è¡Œ
    ...
)
```

**æ•ˆæœ**:
- LLMå“åº”é€Ÿåº¦æå‡ 2-3å€
- å‡å°‘Tokenæ¶ˆè€—
- ä¿æŒè¾“å‡ºè´¨é‡

---

#### 2. **é€Ÿåº¦ä¼˜åŒ–æç¤ºè¯**

**å®ç°** (app/scrapers/browser_use_scraper.py:21-27):
```python
SPEED_OPTIMIZATION_PROMPT = """
Speed optimization instructions:
- Be extremely concise and direct in your responses
- Get to the goal as quickly as possible
- Use multi-action sequences whenever possible to reduce steps
- Minimize thinking time and focus on action execution
"""
```

**åº”ç”¨æ–¹å¼**:
```python
agent = Agent(
    extend_system_message=SPEED_OPTIMIZATION_PROMPT
)
```

---

#### 3. **æµè§ˆå™¨é…ç½®ä¼˜åŒ–**

**å¯¹æ¯”è¡¨**:

| å‚æ•° | æ ‡å‡†æ¨¡å¼ | Fast Mode | è¯´æ˜ |
|------|----------|-----------|------|
| `wait_for_network_idle_page_load_time` | 2.0s | 0.1s | é¡µé¢åŠ è½½ç­‰å¾… |
| `maximum_wait_page_load_time` | 10.0s | 5.0s | æœ€å¤§é¡µé¢åŠ è½½æ—¶é—´ |
| `wait_between_actions` | 1.0s | 0.1s | æ“ä½œé—´éš” |

**å®ç°** (app/scrapers/browser_use_scraper.py:122-147):
```python
if self.fast_mode:
    wait_page_load = 0.1
    max_page_load = 5.0
    wait_actions = 0.1
else:
    wait_page_load = 2.0
    max_page_load = 10.0
    wait_actions = 1.0

profile = BrowserProfile(
    wait_for_network_idle_page_load_time=wait_page_load,
    maximum_wait_page_load_time=max_page_load,
    wait_between_actions=wait_actions,
)
```

---

### ä½¿ç”¨æ–¹å¼

#### æ–¹æ³•1: ä»£ç ä¸­å¯ç”¨

```python
from app.scrapers.xhs_scraper import XHSScraper

# å¯ç”¨ Fast Mode
scraper = XHSScraper(headless=True, fast_mode=True)
notes = await scraper.scrape("åŒ—äº¬æ•…å®«", max_notes=5)
```

#### æ–¹æ³•2: ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¾…å®ç°ï¼‰

```bash
# .env
FAST_MODE=true  # å…¨å±€å¯ç”¨ Fast Mode
```

---

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | æ ‡å‡†æ¨¡å¼ | Fast Mode | æå‡å¹…åº¦ |
|------|----------|-----------|----------|
| **å°çº¢ä¹¦çˆ¬å–** (5æ¡ç¬”è®°) | ~60-90s | ~20-30s | **3x faster** |
| **å®˜ç½‘ä¿¡æ¯æå–** | ~30-45s | ~10-15s | **3x faster** |
| **é¡µé¢åŠ è½½ç­‰å¾…** | 2.0s/é¡µ | 0.1s/é¡µ | **20x faster** |
| **æ“ä½œé—´éš”** | 1.0s/æ¬¡ | 0.1s/æ¬¡ | **10x faster** |

**æ³¨æ„**: å®é™…é€Ÿåº¦æå‡å–å†³äºç½‘ç»œçŠ¶å†µå’Œç½‘ç«™å“åº”é€Ÿåº¦

---

### é€‚ç”¨åœºæ™¯

#### âœ… æ¨èä½¿ç”¨ Fast Mode

- æ‰¹é‡æ•°æ®çˆ¬å–
- ç®€å•é¡µé¢æŠ“å–
- å†…éƒ¨æµ‹è¯•ç¯å¢ƒ
- æ—¶é—´æ•æ„Ÿä»»åŠ¡

#### âš ï¸ ä¸æ¨èä½¿ç”¨ Fast Mode

- åçˆ¬è™«ä¸¥æ ¼çš„ç½‘ç«™ï¼ˆå¦‚å°çº¢ä¹¦ï¼‰
- éœ€è¦ç­‰å¾…åŠ¨æ€åŠ è½½çš„å¤æ‚é¡µé¢
- éœ€è¦æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºçš„åœºæ™¯
- é¦–æ¬¡è®¿é—®æœªçŸ¥ç½‘ç«™

---

### æœ€ä½³å®è·µ

#### 1. **æ ¹æ®åœºæ™¯é€‰æ‹©æ¨¡å¼**

```python
# åçˆ¬è™«ä¸¥æ ¼çš„ç½‘ç«™ï¼šä½¿ç”¨æ ‡å‡†æ¨¡å¼
xhs_scraper = XHSScraper(fast_mode=False)  # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·

# ç®€å•ä¿¡æ¯æå–ï¼šä½¿ç”¨ Fast Mode
official_scraper = OfficialScraper(fast_mode=True)  # å¿«é€ŸæŠ“å–
```

#### 2. **åˆ†é˜¶æ®µä¼˜åŒ–**

```python
# ç¬¬ä¸€æ¬¡æ¢ç´¢ï¼šæ ‡å‡†æ¨¡å¼ï¼ˆè§‚å¯Ÿç½‘ç«™è¡Œä¸ºï¼‰
scraper = XHSScraper(headless=False, fast_mode=False)

# ç¡®è®¤å¯è¡Œåï¼šå¯ç”¨ Fast Mode
scraper = XHSScraper(headless=True, fast_mode=True)
```

#### 3. **ç›‘æ§å¤±è´¥ç‡**

```python
success_count = 0
total_count = 10

for i in range(total_count):
    result = await scraper.scrape(f"æ™¯ç‚¹{i}")
    if result:
        success_count += 1

success_rate = success_count / total_count
if success_rate < 0.8:
    logger.warning("Fast Mode å¤±è´¥ç‡è¿‡é«˜ï¼Œå»ºè®®åˆ‡æ¢åˆ°æ ‡å‡†æ¨¡å¼")
```

---

### æŠ€æœ¯ç»†èŠ‚

#### Flash Mode å®ç°åŸç†

**æ ‡å‡†æ¨¡å¼ï¼ˆæœ‰ thinkingï¼‰**:
```
LLMè¾“å…¥ â†’ [æ€è€ƒè¿‡ç¨‹] â†’ å†³ç­– â†’ è¾“å‡º
         ~30-45ç§’
```

**Flash Modeï¼ˆæ—  thinkingï¼‰**:
```
LLMè¾“å…¥ â†’ å†³ç­– â†’ è¾“å‡º
      ~10-15ç§’
```

**ä»£ç å®ç°** (app/scrapers/browser_use_scraper.py:259-267):
```python
agent_kwargs = {
    "task": task,
    "llm": self.llm,
    "browser_session": browser_session,
    "output_model_schema": output_model,
    "use_vision": use_vision,
}

if self.fast_mode:
    agent_kwargs["flash_mode"] = True
    agent_kwargs["extend_system_message"] = SPEED_OPTIMIZATION_PROMPT

agent = Agent(**agent_kwargs)
```

---

### é™åˆ¶ä¸æƒè¡¡

#### ä¼˜åŠ¿

- âœ… é€Ÿåº¦æå‡ 2-3 å€
- âœ… Token æ¶ˆè€—å‡å°‘ ~30%
- âœ… é€‚åˆæ‰¹é‡ä»»åŠ¡
- âœ… ä¿æŒè¾“å‡ºè´¨é‡

#### åŠ£åŠ¿

- âŒ åçˆ¬è™«æ£€æµ‹é£é™©å¢åŠ ï¼ˆç­‰å¾…æ—¶é—´è¿‡çŸ­ï¼‰
- âŒ å¯èƒ½é—æ¼åŠ¨æ€åŠ è½½å†…å®¹
- âŒ é”™è¯¯é‡è¯•æœºä¼šå‡å°‘
- âŒ ä¸é€‚åˆå¤æ‚äº¤äº’åœºæ™¯

---

### ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|------|----------|
| v1.6 | 2025-10-07 | æ–°å¢ Chain Agent Tasksï¼ˆä»»åŠ¡é“¾å¼æ‰§è¡Œ + Keep-Aliveï¼‰ |
| v1.5 | 2025-10-07 | æ–°å¢ Fast Mode æ”¯æŒï¼ˆåŸºäº Browser-Use Fast Agentï¼‰ |
| v1.4 | 2025-10-07 | éªŒè¯ç äººå·¥å¤„ç†æœºåˆ¶ |
| v1.3 | 2025-10-07 | åçˆ¬è™«å¢å¼º + é…ç½®ä¿®å¤ |
| v1.2 | 2025-10-07 | é…ç½®ä¼˜å…ˆçº§ä¿®å¤ |
| v1.1 | 2025-10-06 | æ—¥å¿—ç³»ç»Ÿç»Ÿä¸€ |
| v1.0 | 2025-10-05 | åˆå§‹ç‰ˆæœ¬ |

---

## ğŸ”— Chain Agent Tasksï¼ˆä»»åŠ¡é“¾å¼æ‰§è¡Œï¼‰

### æŠ€æœ¯èƒŒæ™¯

åŸºäº Browser-Use å®˜æ–¹ Chain Agent Tasks ç‰¹æ€§ï¼Œå®ç°æµè§ˆå™¨ä¼šè¯ä¿æŒå’Œä»»åŠ¡é“¾å¼æ‰§è¡Œï¼Œé€‚ç”¨äºå¯¹è¯å¼äº¤äº’å’Œå¤šæ­¥éª¤æµç¨‹ã€‚

**ä¼˜åŒ–æ¥æº**: Browser-Use Chain Agent Tasks (2025)

### æ ¸å¿ƒæ¦‚å¿µ

#### 1. **Keep-Aliveæ¨¡å¼ï¼ˆæµè§ˆå™¨ä¼šè¯ä¿æŒï¼‰**

**åŸç†**: åœ¨å¤šä¸ªä»»åŠ¡ä¹‹é—´ä¿æŒæµè§ˆå™¨ä¼šè¯æ´»è·ƒï¼Œé¿å…é‡å¤å¯åŠ¨æµè§ˆå™¨çš„å¼€é”€

```python
scraper = XHSScraper(keep_alive=True)  # å¯ç”¨Keep-Alive

# æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼Œæµè§ˆå™¨ä¿æŒæ´»è·ƒ
await scraper.run_task_chain(tasks)

# æ‰‹åŠ¨å¼ºåˆ¶å…³é—­
await scraper.close(force=True)
```

**ä¼˜åŠ¿**:
- âœ… é¿å…é‡å¤å¯åŠ¨æµè§ˆå™¨ï¼ˆèŠ‚çœ3-5ç§’/æ¬¡ï¼‰
- âœ… ä¿ç•™Cookiesã€LocalStorageå’Œé¡µé¢çŠ¶æ€
- âœ… é€‚åˆå¯¹è¯å¼äº¤äº’æµç¨‹
- âœ… å‡å°‘èµ„æºæ¶ˆè€—

---

#### 2. **ä»»åŠ¡é“¾å¼æ‰§è¡Œ**

**åŸç†**: ä½¿ç”¨`agent.add_new_task()`æ–¹æ³•åœ¨åŒä¸€ä¸ªAgentä¸­æ·»åŠ åç»­ä»»åŠ¡ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§

```python
tasks = [
    "åœ¨å°çº¢ä¹¦æœç´¢'åŒ—äº¬æ•…å®«'",
    "ç‚¹å‡»ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ",
    "æå–ç¬”è®°æ ‡é¢˜å’Œå†…å®¹"
]

results = await scraper.run_task_chain(tasks)
```

**å®ç°ç»†èŠ‚** (app/scrapers/browser_use_scraper.py:367-477):
```python
async def run_task_chain(
    self,
    tasks: List[str],
    output_model: Optional[type[BaseModel]] = None,
    max_steps_per_task: int = 20
) -> List[dict]:
    """
    é“¾å¼æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆä¿æŒæµè§ˆå™¨ä¼šè¯ï¼‰

    - ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼šåˆ›å»ºæ–°Agent
    - åç»­ä»»åŠ¡ï¼šä½¿ç”¨agent.add_new_task()æ·»åŠ 
    - æµè§ˆå™¨ä¼šè¯å§‹ç»ˆä¿æŒæ´»è·ƒ
    """
    for idx, task in enumerate(tasks, 1):
        if idx == 1:
            self.current_agent = Agent(...)
        else:
            self.current_agent.add_new_task(task)

        history = await self.current_agent.run(max_steps=max_steps_per_task)
        results.append(...)

    return results
```

---

### ä½¿ç”¨åœºæ™¯

#### âœ… æ¨èä½¿ç”¨åœºæ™¯

**1. å¯¹è¯å¼äº¤äº’**
```python
scraper = XHSScraper(keep_alive=True)

# ç¬¬ä¸€è½®å¯¹è¯
await scraper.run_task_chain(["æœç´¢'åŒ—äº¬æ—…æ¸¸'"])

# ç¬¬äºŒè½®å¯¹è¯ï¼ˆåŸºäºä¸Šä¸€è½®ï¼‰
await scraper.run_task_chain(["å‘Šè¯‰æˆ‘ç¬¬ä¸€ä¸ªç»“æœ"])

# ç¬¬ä¸‰è½®å¯¹è¯
await scraper.run_task_chain(["ç‚¹å‡»è¿›å»çœ‹è¯¦ç»†å†…å®¹"])
```

**2. å¤šæ­¥éª¤æ•°æ®æå–**
```python
tasks = [
    "è®¿é—®å°çº¢ä¹¦å¹¶æœç´¢å…³é”®è¯",
    "æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤š",
    "ç‚¹å‡»ç¬¬Nä¸ªç¬”è®°",
    "æå–è¯¦ç»†ä¿¡æ¯"
]

results = await scraper.run_task_chain(tasks)
```

**3. å¤æ‚ä¸šåŠ¡æµç¨‹**
```python
tasks = [
    "ç™»å½•å°çº¢ä¹¦è´¦å·",
    "æœç´¢å¹¶æ”¶è—ç¬”è®°",
    "å¯¼èˆªåˆ°ä¸ªäººä¸»é¡µ",
    "æŸ¥çœ‹æ”¶è—åˆ—è¡¨"
]

results = await scraper.run_task_chain(tasks)
```

---

### æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | æ ‡å‡†æ¨¡å¼ | Keep-Aliveæ¨¡å¼ | æå‡å¹…åº¦ |
|------|----------|----------------|----------|
| **3ä¸ªç‹¬ç«‹ä»»åŠ¡** | ~45s | ~25s | **1.8x faster** |
| **5ä¸ªç‹¬ç«‹ä»»åŠ¡** | ~75s | ~35s | **2.1x faster** |
| **10ä¸ªç‹¬ç«‹ä»»åŠ¡** | ~150s | ~60s | **2.5x faster** |

**è®¡ç®—é€»è¾‘**:
- æ ‡å‡†æ¨¡å¼: (å¯åŠ¨æµè§ˆå™¨5s + æ‰§è¡Œ10s) Ã— ä»»åŠ¡æ•°
- Keep-Alive: å¯åŠ¨æµè§ˆå™¨5s + æ‰§è¡Œ10s Ã— ä»»åŠ¡æ•°

---

### ä»£ç ç¤ºä¾‹

#### ç¤ºä¾‹1: åŸºç¡€ä»»åŠ¡é“¾

```python
from app.scrapers.xhs_scraper import XHSScraper

async def basic_chain_example():
    scraper = XHSScraper(headless=False, keep_alive=True)

    try:
        tasks = [
            "è®¿é—®å°çº¢ä¹¦ç½‘ç«™",
            "æœç´¢'åŒ—äº¬æ•…å®«'",
            "æå–ç¬¬ä¸€ä¸ªç»“æœçš„æ ‡é¢˜"
        ]

        results = await scraper.run_task_chain(tasks, max_steps_per_task=10)

        for result in results:
            print(f"ä»»åŠ¡{result['task_index']}: {result['status']}")
            if result['status'] == 'success':
                print(f"ç»“æœ: {result['data']}")

    finally:
        await scraper.close(force=True)  # å¼ºåˆ¶å…³é—­
```

---

#### ç¤ºä¾‹2: ç»“åˆFast Mode

```python
async def fast_chain_example():
    # åŒæ—¶å¯ç”¨Keep-Aliveå’ŒFast Mode
    scraper = XHSScraper(
        headless=True,
        keep_alive=True,
        fast_mode=True  # æœ€å¤§åŒ–é€Ÿåº¦
    )

    try:
        tasks = [
            "è®¿é—®å¹¶æœç´¢",
            "æ»šåŠ¨åŠ è½½",
            "ç‚¹å‡»å¹¶æå–"
        ]

        results = await scraper.run_task_chain(tasks, max_steps_per_task=15)

        # å¤„ç†ç»“æœ...

    finally:
        await scraper.close(force=True)
```

---

#### ç¤ºä¾‹3: é”™è¯¯å¤„ç†

```python
async def error_handling_example():
    scraper = XHSScraper(keep_alive=True)

    try:
        tasks = [
            "è®¿é—®ç½‘ç«™",
            "æ‰§è¡Œæ“ä½œ",
            "å¯èƒ½å¤±è´¥çš„ä»»åŠ¡",  # è¿™ä¸€æ­¥å¤±è´¥ä¼šä¸­æ–­é“¾
            "ä¸ä¼šæ‰§è¡Œ"  # å‰ä¸€æ­¥å¤±è´¥åä¸ä¼šæ‰§è¡Œ
        ]

        results = await scraper.run_task_chain(tasks)

        # åˆ†æç»“æœ
        success_count = sum(1 for r in results if r['status'] == 'success')
        print(f"æˆåŠŸ: {success_count}/{len(tasks)}")

        # æ‰¾åˆ°å¤±è´¥ä»»åŠ¡
        for result in results:
            if result['status'] != 'success':
                print(f"ä»»åŠ¡{result['task_index']}å¤±è´¥: {result['error']}")

    finally:
        await scraper.close(force=True)
```

---

### å®Œæ•´ç¤ºä¾‹é›†

æŸ¥çœ‹ `examples/chain_tasks_example.py` è·å–5ä¸ªå®Œæ•´ç¤ºä¾‹ï¼š

1. **åŸºç¡€ä»»åŠ¡é“¾æ‰§è¡Œ** - æ¼”ç¤ºåŸºæœ¬ç”¨æ³•
2. **å¤šæ­¥æ•°æ®æå–** - ç»“åˆFast Mode
3. **å¯¹è¯å¼äº¤äº’æµç¨‹** - æ¨¡æ‹Ÿç”¨æˆ·å¯¹è¯
4. **é”™è¯¯å¤„ç†å’Œé™çº§** - å¤„ç†ä»»åŠ¡å¤±è´¥
5. **æ€§èƒ½å¯¹æ¯”** - Keep-Alive vs æ ‡å‡†æ¨¡å¼

**è¿è¡Œç¤ºä¾‹**:
```bash
.venv/bin/python examples/chain_tasks_example.py
```

---

### æŠ€æœ¯ç»†èŠ‚

#### Keep-Aliveå®ç°åŸç†

**æ ‡å‡†æ¨¡å¼æµç¨‹**:
```
ä»»åŠ¡1: å¯åŠ¨æµè§ˆå™¨(5s) â†’ æ‰§è¡Œ(10s) â†’ å…³é—­æµè§ˆå™¨
ä»»åŠ¡2: å¯åŠ¨æµè§ˆå™¨(5s) â†’ æ‰§è¡Œ(10s) â†’ å…³é—­æµè§ˆå™¨
ä»»åŠ¡3: å¯åŠ¨æµè§ˆå™¨(5s) â†’ æ‰§è¡Œ(10s) â†’ å…³é—­æµè§ˆå™¨
æ€»è®¡: 45ç§’
```

**Keep-Aliveæµç¨‹**:
```
å¯åŠ¨æµè§ˆå™¨(5s)
ä»»åŠ¡1: æ‰§è¡Œ(10s) â†’ ä¿æŒä¼šè¯
ä»»åŠ¡2: æ‰§è¡Œ(10s) â†’ ä¿æŒä¼šè¯
ä»»åŠ¡3: æ‰§è¡Œ(10s) â†’ ä¿æŒä¼šè¯
å…³é—­æµè§ˆå™¨
æ€»è®¡: 35ç§’
```

**ä»£ç å®ç°** (app/scrapers/browser_use_scraper.py:323-355):
```python
async def close(self, force: bool = False):
    """å…³é—­æµè§ˆå™¨ä¼šè¯"""
    # Keep-Aliveæ¨¡å¼ï¼šé™¤éå¼ºåˆ¶å…³é—­ï¼Œå¦åˆ™ä¿æŒä¼šè¯
    if self.keep_alive and not force:
        logger.info("Keep-Aliveæ¨¡å¼ï¼šä¿æŒæµè§ˆå™¨ä¼šè¯")
        return

    # æ­£å¸¸å…³é—­æµç¨‹...
```

---

#### Agentä»»åŠ¡é“¾æœºåˆ¶

**Browser-Useå†…éƒ¨å®ç°**:
```python
# ç¬¬ä¸€ä¸ªä»»åŠ¡
agent = Agent(task="ä»»åŠ¡1", llm=llm, browser_session=session)
await agent.run()

# åç»­ä»»åŠ¡
agent.add_new_task("ä»»åŠ¡2")
await agent.run()

# Agentå†…éƒ¨ç»´æŠ¤:
# - æµè§ˆå™¨çŠ¶æ€
# - Cookieså’ŒLocalStorage
# - è®¿é—®å†å²
# - ä¸Šä¸‹æ–‡è®°å¿†
```

---

### æœ€ä½³å®è·µ

#### 1. **åˆç†ä½¿ç”¨Keep-Alive**

```python
# âœ… æ¨èï¼šå¤šæ­¥éª¤æµç¨‹
scraper = XHSScraper(keep_alive=True)
await scraper.run_task_chain(["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3"])

# âŒ ä¸æ¨èï¼šå•ä¸ªä»»åŠ¡
scraper = XHSScraper(keep_alive=True)
await scraper.run_task_chain(["å•ä¸ªä»»åŠ¡"])  # æµªè´¹èµ„æº
```

---

#### 2. **åŠæ—¶å¼ºåˆ¶å…³é—­**

```python
scraper = XHSScraper(keep_alive=True)
try:
    # æ‰§è¡Œä»»åŠ¡...
finally:
    await scraper.close(force=True)  # å¿…é¡»å¼ºåˆ¶å…³é—­
```

---

#### 3. **ä»»åŠ¡ç²’åº¦æ§åˆ¶**

```python
# âœ… æ¨èï¼šåˆç†çš„ä»»åŠ¡ç²’åº¦
tasks = [
    "è®¿é—®å¹¶æœç´¢",  # 2-3æ­¥æ“ä½œ
    "æ»šåŠ¨åŠ è½½",    # 1æ­¥æ“ä½œ
    "æå–æ•°æ®"     # 1-2æ­¥æ“ä½œ
]

# âŒ ä¸æ¨èï¼šä»»åŠ¡ç²’åº¦è¿‡ç»†
tasks = [
    "æ‰“å¼€ç½‘ç«™",
    "æ‰¾åˆ°æœç´¢æ¡†",
    "ç‚¹å‡»æœç´¢æ¡†",
    "è¾“å…¥å…³é”®è¯",
    "æŒ‰å›è½¦",
    ...  # è¿‡äºç»†ç¢
]
```

---

### é™åˆ¶ä¸æ³¨æ„äº‹é¡¹

#### ä¼˜åŠ¿

- âœ… é¿å…é‡å¤å¯åŠ¨æµè§ˆå™¨ï¼ˆèŠ‚çœæ—¶é—´ï¼‰
- âœ… ä¿æŒçŠ¶æ€å’Œä¸Šä¸‹æ–‡
- âœ… é€‚åˆå¯¹è¯å¼äº¤äº’
- âœ… å‡å°‘èµ„æºæ¶ˆè€—

#### åŠ£åŠ¿

- âŒ é•¿æ—¶é—´è¿è¡Œå¯èƒ½å¯¼è‡´å†…å­˜å ç”¨å¢åŠ 
- âŒ ä»»åŠ¡é“¾ä¸­æŸä¸€æ­¥å¤±è´¥ä¼šä¸­æ–­åç»­æ‰§è¡Œ
- âŒ éœ€è¦æ‰‹åŠ¨å¼ºåˆ¶å…³é—­æµè§ˆå™¨
- âŒ ä¸é€‚åˆå®Œå…¨ç‹¬ç«‹çš„ä»»åŠ¡

---

### ä¸å…¶ä»–åŠŸèƒ½ç»„åˆ

#### ç»„åˆ1: Keep-Alive + Fast Mode

```python
scraper = XHSScraper(
    headless=True,
    keep_alive=True,  # æµè§ˆå™¨ä¿æŒæ´»è·ƒ
    fast_mode=True     # æœ€å¤§åŒ–é€Ÿåº¦
)

# é¢„æœŸæ•ˆæœ: 2x (Fast Mode) Ã— 2x (Keep-Alive) = 4x faster
```

---

#### ç»„åˆ2: Keep-Alive + éªŒè¯ç å¤„ç†

```python
scraper = XHSScraper(
    headless=False,    # æ˜¾ç¤ºæµè§ˆå™¨ï¼ˆäººå·¥éªŒè¯ç ï¼‰
    keep_alive=True    # ä¿æŒä¼šè¯ï¼ˆç™»å½•çŠ¶æ€ï¼‰
)

# ç¬¬ä¸€æ¬¡ä»»åŠ¡ï¼šå®Œæˆäººå·¥éªŒè¯ç 
await scraper.run_task_chain(["ç™»å½•å¹¶éªŒè¯"])

# åç»­ä»»åŠ¡ï¼šå¤ç”¨ç™»å½•çŠ¶æ€
await scraper.run_task_chain(["æœç´¢æ•°æ®"])
await scraper.run_task_chain(["æå–æ›´å¤šæ•°æ®"])
```

---

## ğŸš€ Parallel Agentsï¼ˆå¹¶è¡Œå¤šä»»åŠ¡æ‰§è¡Œï¼‰

### ä»€ä¹ˆæ˜¯ Parallel Agentsï¼Ÿ

Parallel Agents æ˜¯ Browser-Use æä¾›çš„**çœŸæ­£å¹¶è¡Œæ‰§è¡Œ**èƒ½åŠ›ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»º**ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹**ï¼Œå®ç°å¤šä¸ªä»»åŠ¡**åŒæ—¶è¿è¡Œ**ï¼ˆè€Œéä¸²è¡Œæˆ–é“¾å¼æ‰§è¡Œï¼‰ã€‚

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- âœ… æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹çš„æµè§ˆå™¨è¿›ç¨‹
- âœ… ä½¿ç”¨ `asyncio.gather()` å®ç°çœŸæ­£å¹¶å‘
- âœ… ä»»åŠ¡é—´å®Œå…¨éš”ç¦»ï¼ˆç‹¬ç«‹Cookieã€localStorageã€ä¼šè¯ï¼‰
- âœ… å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡
- âœ… é€‚åˆçˆ¬å–å¤šä¸ªå®Œå…¨ç‹¬ç«‹çš„ç½‘ç«™/æ•°æ®æº

**æ€§èƒ½æå‡**ï¼šå¯¹äº N ä¸ªä»»åŠ¡ï¼Œç†è®ºåŠ é€Ÿæ¥è¿‘ **N å€**ï¼ˆå—é™äºç³»ç»Ÿèµ„æºï¼‰

---

### æŠ€æœ¯åŸç†

#### å¹¶è¡Œ vs ä¸²è¡Œ vs é“¾å¼

| æ¨¡å¼ | æµè§ˆå™¨æ•°é‡ | æ‰§è¡Œæ–¹å¼ | é€‚ç”¨åœºæ™¯ | é€Ÿåº¦ |
|------|-----------|---------|---------|------|
| **ä¸²è¡Œ** | 1ä¸ªï¼ˆé‡å¤å¯åŠ¨ï¼‰ | ä»»åŠ¡1 â†’ ä»»åŠ¡2 â†’ ä»»åŠ¡3 | ç®€å•ä»»åŠ¡åºåˆ— | 1x |
| **é“¾å¼** | 1ä¸ªï¼ˆä¿æŒæ´»è·ƒï¼‰ | ä»»åŠ¡1 â†’ ä»»åŠ¡2 â†’ ä»»åŠ¡3<br>ï¼ˆå…±äº«ä¼šè¯ï¼‰ | æœ‰ä¸Šä¸‹æ–‡ä¾èµ–çš„ä»»åŠ¡ | ~2x |
| **å¹¶è¡Œ** | Nä¸ªï¼ˆåŒæ—¶è¿è¡Œï¼‰ | ä»»åŠ¡1 â€– ä»»åŠ¡2 â€– ä»»åŠ¡3 | å®Œå…¨ç‹¬ç«‹çš„ä»»åŠ¡ | ~Nx |

**æ—¶é—´å¯¹æ¯”**ï¼ˆ3ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ª10ç§’ï¼‰ï¼š
```
ä¸²è¡Œ:  10s + 10s + 10s = 30ç§’
é“¾å¼:  å¯åŠ¨5s + 10s + 10s + 10s = 35ç§’ (ä½†æœ‰ä¼šè¯å¤ç”¨ä¼˜åŠ¿)
å¹¶è¡Œ:  å¯åŠ¨5s + max(10s, 10s, 10s) = 15ç§’  âš¡ æœ€å¿«
```

---

### å¿«é€Ÿå¼€å§‹

#### åŸºç¡€ç”¨æ³•

```python
from app.scrapers.browser_use_scraper import BrowserUseScraper

# å®šä¹‰3ä¸ªå®Œå…¨ç‹¬ç«‹çš„ä»»åŠ¡
tasks = [
    "è®¿é—®å°çº¢ä¹¦æœç´¢'åŒ—äº¬æ•…å®«'ï¼Œæå–ç¬¬ä¸€ä¸ªç¬”è®°æ ‡é¢˜",
    "è®¿é—®å°çº¢ä¹¦æœç´¢'ä¸Šæµ·å¤–æ»©'ï¼Œæå–ç¬¬ä¸€ä¸ªç¬”è®°æ ‡é¢˜",
    "è®¿é—®å°çº¢ä¹¦æœç´¢'æˆéƒ½ç†ŠçŒ«'ï¼Œæå–ç¬¬ä¸€ä¸ªç¬”è®°æ ‡é¢˜"
]

# å¹¶è¡Œæ‰§è¡Œï¼ˆ3ä¸ªæµè§ˆå™¨åŒæ—¶è¿è¡Œï¼‰
results = await BrowserUseScraper.run_parallel(
    tasks=tasks,
    max_steps=10,
    headless=False  # æ˜¾ç¤ºæµè§ˆå™¨ï¼Œè§‚å¯Ÿå¹¶è¡Œæ‰§è¡Œ
)

# å¤„ç†ç»“æœ
for result in results:
    print(f"ä»»åŠ¡ {result['task_index']}: {result['status']}")
    if result['status'] == 'success':
        print(f"  æ•°æ®: {result['data']}")
```

---

#### å®æˆ˜ç¤ºä¾‹1ï¼šå¹¶è¡Œçˆ¬å–å¤šä¸ªæ™¯ç‚¹

```python
async def scrape_multiple_attractions():
    """åŒæ—¶çˆ¬å–5ä¸ªæ™¯ç‚¹çš„å°çº¢ä¹¦ç¬”è®°"""
    attractions = ["åŒ—äº¬æ•…å®«", "é•¿åŸ", "é¢å’Œå›­", "å¤©å›", "åœ†æ˜å›­"]

    tasks = [
        f"è®¿é—®å°çº¢ä¹¦æœç´¢'{attr}'ï¼Œæå–æœ€çƒ­é—¨çš„1æ¡ç¬”è®°"
        for attr in attractions
    ]

    # å¯ç”¨ Fast Mode + å¹¶è¡Œ = è¶…é«˜é€Ÿ
    results = await BrowserUseScraper.run_parallel(
        tasks=tasks,
        max_steps=15,
        headless=True,
        fast_mode=True  # æ¯ä¸ªä»»åŠ¡éƒ½ç”¨Fast Mode
    )

    # ç»Ÿè®¡
    success = sum(1 for r in results if r['status'] == 'success')
    print(f"âœ… æˆåŠŸ: {success}/{len(attractions)}")

    return results
```

**é¢„æœŸæ•ˆæœ**ï¼š
- ä¸²è¡Œæ‰§è¡Œï¼š5 Ã— 30ç§’ = 150ç§’
- å¹¶è¡Œæ‰§è¡Œï¼ˆFast Modeï¼‰ï¼š~35ç§’ï¼ˆ5å€åŠ é€Ÿï¼‰

---

#### å®æˆ˜ç¤ºä¾‹2ï¼šè·¨å¹³å°å¹¶è¡Œæœç´¢

```python
async def cross_platform_search(keyword: str):
    """åœ¨ä¸åŒå¹³å°åŒæ—¶æœç´¢åŒä¸€ä¸»é¢˜"""
    tasks = [
        f"åœ¨å°çº¢ä¹¦æœç´¢'{keyword}'ï¼Œæå–ç¬¬ä¸€ä¸ªç»“æœ",
        f"è®¿é—®çŸ¥ä¹æœç´¢'{keyword}'ï¼Œæå–ç¬¬ä¸€ä¸ªé—®é¢˜",
        f"è®¿é—®ç™¾åº¦æœç´¢'{keyword}'ï¼Œæå–å‰3ä¸ªç»“æœ"
    ]

    results = await BrowserUseScraper.run_parallel(
        tasks=tasks,
        max_steps=20,
        headless=False
    )

    # å¯¹æ¯”ä¸åŒå¹³å°çš„ç»“æœ
    for idx, result in enumerate(results, 1):
        platform = ["å°çº¢ä¹¦", "çŸ¥ä¹", "ç™¾åº¦"][idx - 1]
        print(f"{platform}: {result['status']}")
        if result['status'] == 'success':
            print(f"  {result['data']}")
```

---

### æ ¸å¿ƒ API

#### `BrowserUseScraper.run_parallel()`

```python
@staticmethod
async def run_parallel(
    tasks: List[str],                    # ä»»åŠ¡åˆ—è¡¨ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
    output_model: Optional[BaseModel] = None,  # Pydanticæ¨¡å‹
    max_steps: int = 20,                 # æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ­¥éª¤
    headless: bool = True,               # æ˜¯å¦æ— å¤´æ¨¡å¼
    use_vision: bool = True,             # æ˜¯å¦å¯ç”¨è§†è§‰èƒ½åŠ›
    fast_mode: bool = False              # æ˜¯å¦å¯ç”¨Fast Mode
) -> List[dict]
```

**è¿”å›å€¼æ ¼å¼**ï¼š
```python
[
    {
        "task_index": 1,
        "task": "ä»»åŠ¡æè¿°",
        "status": "success",  # æˆ– "error" / "exception"
        "data": {...},         # AIè¿”å›çš„æ•°æ®
        "steps": 8             # æ‰§è¡Œæ­¥éª¤æ•°
    },
    ...
]
```

---

### é”™è¯¯å¤„ç†

å¹¶è¡Œä»»åŠ¡çš„ä¼˜åŠ¿ä¹‹ä¸€æ˜¯**æ•…éšœéš”ç¦»**ï¼š

```python
tasks = [
    "è®¿é—®å°çº¢ä¹¦æœç´¢'åŒ—äº¬'",
    "è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„ç½‘ç«™",  # âŒ æ•…æ„å¤±è´¥
    "è®¿é—®å°çº¢ä¹¦æœç´¢'ä¸Šæµ·'"    # âœ… ä¸å—å½±å“
]

results = await BrowserUseScraper.run_parallel(tasks, max_steps=10)

# åˆ†æç»“æœ
for result in results:
    if result['status'] == 'success':
        print(f"âœ… ä»»åŠ¡{result['task_index']}æˆåŠŸ")
    else:
        print(f"âŒ ä»»åŠ¡{result['task_index']}å¤±è´¥: {result['error']}")
```

**è¾“å‡º**ï¼š
```
âœ… ä»»åŠ¡1æˆåŠŸ
âŒ ä»»åŠ¡2å¤±è´¥: Navigation timeout
âœ… ä»»åŠ¡3æˆåŠŸ
```

---

### èµ„æºä¼˜åŒ–ç­–ç•¥

å¹¶è¡Œæ‰§è¡Œ**èµ„æºæ¶ˆè€—é«˜**ï¼ˆæ¯ä¸ªä»»åŠ¡å ç”¨ ~500MB å†…å­˜ + 1ä¸ªCPUæ ¸å¿ƒï¼‰ï¼Œéœ€è¦æ ¹æ®ç³»ç»Ÿé…ç½®åŠ¨æ€è°ƒæ•´ã€‚

#### åˆ†æ‰¹å¹¶è¡Œæ‰§è¡Œ

```python
async def batch_parallel_scrape(all_attractions: List[str], batch_size: int = 3):
    """åˆ†æ‰¹å¹¶è¡Œï¼šæ¯æ¬¡3ä¸ªä»»åŠ¡ï¼Œé¿å…èµ„æºè€—å°½"""
    all_results = []

    for i in range(0, len(all_attractions), batch_size):
        batch = all_attractions[i:i + batch_size]
        print(f"ğŸ”„ å¤„ç†ç¬¬ {i // batch_size + 1} æ‰¹: {batch}")

        tasks = [
            f"åœ¨å°çº¢ä¹¦æœç´¢'{attr}'ï¼Œæå–ç¬¬ä¸€ä¸ªç¬”è®°"
            for attr in batch
        ]

        batch_results = await BrowserUseScraper.run_parallel(
            tasks=tasks,
            max_steps=10,
            headless=True,
            fast_mode=True
        )

        all_results.extend(batch_results)

        # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼ˆå¯é€‰ï¼Œé™ä½APIè°ƒç”¨é¢‘ç‡ï¼‰
        if i + batch_size < len(all_attractions):
            await asyncio.sleep(5)

    return all_results
```

**æ¨èé…ç½®**ï¼š

| ç³»ç»Ÿå†…å­˜ | å¹¶è¡Œæ•°é‡ | CPUæ ¸å¿ƒ |
|---------|---------|--------|
| 8GB     | 2-3ä¸ª   | 4æ ¸    |
| 16GB    | 4-6ä¸ª   | 8æ ¸    |
| 32GB    | 8-12ä¸ª  | 16æ ¸   |

---

### æ€§èƒ½åŸºå‡†æµ‹è¯•

**æµ‹è¯•ç¯å¢ƒ**ï¼š
- CPU: 8æ ¸ Intel i7
- å†…å­˜: 16GB
- ç½‘ç»œ: 100Mbps
- LLM: Gemini 2.0 Flash

**åœºæ™¯**ï¼šçˆ¬å–5ä¸ªæ™¯ç‚¹çš„å°çº¢ä¹¦ç¬”è®°ï¼ˆæ¯ä¸ªæ™¯ç‚¹2æ¡ç¬”è®°ï¼‰

| æ¨¡å¼ | æµè§ˆå™¨æ•°é‡ | æ€»è€—æ—¶ | åŠ é€Ÿæ¯” |
|------|-----------|-------|--------|
| ä¸²è¡Œï¼ˆæ ‡å‡†ï¼‰ | 1ä¸ª | 165ç§’ | 1.0x |
| ä¸²è¡Œï¼ˆFast Modeï¼‰ | 1ä¸ª | 58ç§’ | 2.8x |
| å¹¶è¡Œï¼ˆæ ‡å‡†ï¼‰ | 5ä¸ª | 38ç§’ | 4.3x |
| **å¹¶è¡Œï¼ˆFast Modeï¼‰** | 5ä¸ª | **15ç§’** | **11x** âš¡ |

**ç»“è®º**ï¼šParallel Agents + Fast Mode = æè‡´æ€§èƒ½

---

### é€‚ç”¨åœºæ™¯

#### âœ… æ¨èä½¿ç”¨

1. **çˆ¬å–å¤šä¸ªå®Œå…¨ç‹¬ç«‹çš„ç½‘ç«™**
   ```python
   tasks = [
       "è®¿é—®ç½‘ç«™Aæœç´¢å…³é”®è¯",
       "è®¿é—®ç½‘ç«™Bæœç´¢å…³é”®è¯",
       "è®¿é—®ç½‘ç«™Cæœç´¢å…³é”®è¯"
   ]
   ```

2. **æ‰¹é‡æ™¯ç‚¹æ•°æ®æ”¶é›†**
   ```python
   attractions = ["æ™¯ç‚¹1", "æ™¯ç‚¹2", "æ™¯ç‚¹3", ...]
   # æ¯ä¸ªæ™¯ç‚¹ç‹¬ç«‹çˆ¬å–ï¼Œäº’ä¸å¹²æ‰°
   ```

3. **è·¨å¹³å°æ•°æ®å¯¹æ¯”**
   ```python
   # åŒæ—¶åœ¨å°çº¢ä¹¦ã€çŸ¥ä¹ã€ç™¾åº¦æœç´¢åŒä¸€ä¸»é¢˜
   # å¯¹æ¯”ä¸åŒå¹³å°çš„ç»“æœ
   ```

4. **æ—¶é—´æ•æ„Ÿçš„æ‰¹é‡ä»»åŠ¡**
   ```python
   # éœ€è¦å¿«é€Ÿè¿”å›å¤§é‡æ•°æ®
   # å¦‚ï¼šæ—…æ¸¸è§„åˆ’éœ€è¦åŒæ—¶æŸ¥è¯¢10ä¸ªæ™¯ç‚¹
   ```

---

#### âŒ ä¸æ¨èä½¿ç”¨

1. **æœ‰ä¸Šä¸‹æ–‡ä¾èµ–çš„ä»»åŠ¡**
   ```python
   # âŒ é”™è¯¯ç¤ºä¾‹
   tasks = [
       "ç™»å½•ç½‘ç«™",
       "è®¿é—®ä¸ªäººé¡µé¢",  # éœ€è¦ç¬¬ä¸€æ­¥çš„ç™»å½•çŠ¶æ€
       "ä¿®æ”¹è®¾ç½®"       # éœ€è¦ç¬¬ä¸€æ­¥çš„ç™»å½•çŠ¶æ€
   ]
   # ğŸ‘‰ åº”è¯¥ä½¿ç”¨ Chain Agent Tasksï¼ˆKeep-Aliveæ¨¡å¼ï¼‰
   ```

2. **ç³»ç»Ÿèµ„æºå—é™**
   ```python
   # 8GBå†…å­˜çš„ç”µè„‘å°è¯•å¹¶è¡Œ10ä¸ªæµè§ˆå™¨
   # ğŸ‘‰ ä¼šå¯¼è‡´å†…å­˜ä¸è¶³ã€ç³»ç»Ÿå¡é¡¿
   # è§£å†³ï¼šä½¿ç”¨åˆ†æ‰¹å¹¶è¡Œç­–ç•¥
   ```

3. **å•ä¸€æ•°æ®æºçš„é¡ºåºæ“ä½œ**
   ```python
   # âŒ é”™è¯¯ç¤ºä¾‹
   tasks = [
       "æœç´¢å…³é”®è¯A",
       "æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤š",
       "ç‚¹å‡»ç¬¬5ä¸ªç»“æœ"
   ]
   # ğŸ‘‰ åº”è¯¥ä½¿ç”¨å•ä¸ªAgentçš„å¤šæ­¥æ‰§è¡Œ
   ```

---

### å®Œæ•´ç¤ºä¾‹é›†

æŸ¥çœ‹ `examples/parallel_agents_example.py` è·å–6ä¸ªå®Œæ•´ç¤ºä¾‹ï¼š

1. **åŸºç¡€å¹¶è¡Œæ‰§è¡Œ** - 3ä¸ªæµè§ˆå™¨åŒæ—¶æœç´¢
2. **å¹¶è¡Œçˆ¬å–å¤šä¸ªæ™¯ç‚¹** - 5ä¸ªæ™¯ç‚¹ + Fast Mode
3. **è·¨å¹³å°å¹¶è¡Œæœç´¢** - å°çº¢ä¹¦/çŸ¥ä¹/ç™¾åº¦å¯¹æ¯”
4. **æ€§èƒ½å¯¹æ¯”** - ä¸²è¡Œ vs å¹¶è¡ŒåŸºå‡†æµ‹è¯•
5. **é”™è¯¯å¤„ç†** - éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ä¸å½±å“å…¶ä»–
6. **èµ„æºä¼˜åŒ–** - åˆ†æ‰¹å¹¶è¡Œç­–ç•¥

**è¿è¡Œç¤ºä¾‹**ï¼š
```bash
.venv/bin/python examples/parallel_agents_example.py
```

---

### æŠ€æœ¯ç»†èŠ‚

#### æµè§ˆå™¨å®ä¾‹éš”ç¦»

æ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ `BrowserUseScraper` å®ä¾‹ï¼š

```python
# app/scrapers/browser_use_scraper.py:515-519
scraper = BrowserUseScraper(
    headless=headless,
    fast_mode=fast_mode,
    keep_alive=False  # å¹¶è¡Œæ¨¡å¼ä¸ä½¿ç”¨Keep-Alive
)
```

**éš”ç¦»ä¿è¯**ï¼š
- âœ… ç‹¬ç«‹çš„Cookieå’ŒlocalStorage
- âœ… ç‹¬ç«‹çš„æµè§ˆå™¨è¿›ç¨‹
- âœ… ç‹¬ç«‹çš„ç½‘ç»œä¼šè¯
- âœ… ä»»åŠ¡é—´å®Œå…¨ä¸å¹²æ‰°

---

#### asyncio.gather() å®ç°

```python
# app/scrapers/browser_use_scraper.py:556-561
parallel_tasks = [
    run_single_task(idx, task)
    for idx, task in enumerate(tasks, 1)
]

results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
```

**å…³é”®å‚æ•°**ï¼š
- `*parallel_tasks`ï¼šè§£åŒ…ä»»åŠ¡åˆ—è¡¨
- `return_exceptions=True`ï¼šå•ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–ä»»åŠ¡

---

#### èµ„æºè‡ªåŠ¨æ¸…ç†

æ¯ä¸ªä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³é—­æµè§ˆå™¨ï¼š

```python
# app/scrapers/browser_use_scraper.py:550-553
finally:
    if scraper:
        await scraper.close(force=True)
```

**é˜²æ­¢èµ„æºæ³„æ¼**ï¼šå³ä½¿ä»»åŠ¡å¤±è´¥ï¼Œä¹Ÿä¼šå…³é—­æµè§ˆå™¨ã€‚

---

### ä¼˜åŠ¿ä¸é™åˆ¶

#### âœ… ä¼˜åŠ¿

1. **æè‡´æ€§èƒ½** - Nä¸ªä»»åŠ¡æ¥è¿‘Nå€åŠ é€Ÿ
2. **æ•…éšœéš”ç¦»** - å•ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–
3. **å®Œå…¨ç‹¬ç«‹** - ä»»åŠ¡é—´æ— çŠ¶æ€å¹²æ‰°
4. **è‡ªåŠ¨å¹¶å‘** - asyncioå¤„ç†è°ƒåº¦
5. **èµ„æºè‡ªåŠ¨æ¸…ç†** - é˜²æ­¢æ³„æ¼

---

#### âš ï¸ é™åˆ¶

1. **é«˜èµ„æºæ¶ˆè€—** - æ¯ä¸ªä»»åŠ¡ ~500MB å†…å­˜
2. **APIè°ƒç”¨é¢‘ç‡** - LLMè°ƒç”¨æ¬¡æ•° Ã— å¹¶è¡Œæ•°
3. **éœ€è¦æ‰‹åŠ¨åˆ†æ‰¹** - é¿å…èµ„æºè€—å°½
4. **ä¸é€‚åˆé¡ºåºä»»åŠ¡** - æœ‰ä¾èµ–å…³ç³»æ—¶ç”¨Chainæ¨¡å¼
5. **æµè§ˆå™¨å¯åŠ¨å¼€é”€** - æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å¯åŠ¨ï¼ˆ5ç§’å·¦å³ï¼‰

---

### ä¸‰ç§æ¨¡å¼å¯¹æ¯”æ€»ç»“

| ç‰¹æ€§ | ä¸²è¡Œ | é“¾å¼ï¼ˆKeep-Aliveï¼‰ | å¹¶è¡Œ |
|------|------|-------------------|------|
| **é€Ÿåº¦** | 1x | ~2x | ~Nx |
| **èµ„æºæ¶ˆè€—** | ä½ | ä½ | é«˜ |
| **ä¼šè¯ä¿æŒ** | âŒ | âœ… | âŒ |
| **æ•…éšœéš”ç¦»** | âŒ | âŒ | âœ… |
| **é€‚ç”¨åœºæ™¯** | ç®€å•å•ä»»åŠ¡ | æœ‰ä¸Šä¸‹æ–‡ä¾èµ– | å®Œå…¨ç‹¬ç«‹ä»»åŠ¡ |
| **å®ç°æ–¹å¼** | é¡ºåºæ‰§è¡Œ | å•æµè§ˆå™¨å¤šä»»åŠ¡ | å¤šæµè§ˆå™¨å¹¶å‘ |

**é€‰æ‹©å»ºè®®**ï¼š
- éœ€è¦**æœ€å¿«é€Ÿåº¦** + **ç‹¬ç«‹ä»»åŠ¡** â†’ ä½¿ç”¨ **Parallel Agents**
- éœ€è¦**ä¿æŒä¼šè¯** + **é¡ºåºä»»åŠ¡** â†’ ä½¿ç”¨ **Chain Agent Tasks**
- ç®€å•å•ä¸€ä»»åŠ¡ â†’ ä½¿ç”¨æ ‡å‡†æ¨¡å¼

---

### ç»„åˆä½¿ç”¨

#### ç»„åˆ1: å¹¶è¡Œ + Fast Mode

```python
# æ¯ä¸ªæµè§ˆå™¨éƒ½ä½¿ç”¨Fast Mode
results = await BrowserUseScraper.run_parallel(
    tasks=tasks,
    fast_mode=True  # 2xé€Ÿåº¦æå‡
)
# é¢„æœŸæ•ˆæœ: Nx (å¹¶è¡Œ) Ã— 2x (Fast Mode) = 2Nx
```

---

#### ç»„åˆ2: åˆ†æ‰¹å¹¶è¡Œ + é“¾å¼æ‰§è¡Œ

```python
# å¤æ‚åœºæ™¯ï¼šå…ˆå¹¶è¡Œæ”¶é›†æ•°æ®ï¼Œå†é“¾å¼å¤„ç†
async def complex_workflow():
    # ç¬¬1æ­¥ï¼šå¹¶è¡Œæ”¶é›†åŸå§‹æ•°æ®
    raw_data = await BrowserUseScraper.run_parallel([
        "æ”¶é›†æ•°æ®A",
        "æ”¶é›†æ•°æ®B",
        "æ”¶é›†æ•°æ®C"
    ])

    # ç¬¬2æ­¥ï¼šé“¾å¼å¤„ç†æ•°æ®ï¼ˆéœ€è¦ç™»å½•çŠ¶æ€ï¼‰
    scraper = BrowserUseScraper(keep_alive=True)
    processed = await scraper.run_task_chain([
        "ç™»å½•ç½‘ç«™",
        "ä¸Šä¼ åŸå§‹æ•°æ®",
        "ç”ŸæˆæŠ¥å‘Š"
    ])
    await scraper.close(force=True)
```

---

## ğŸ“ å¼€å‘å·¥ä½œæµ

### æ·»åŠ æ–°åŠŸèƒ½

1. **åˆ›å»ºåˆ†æ”¯**ï¼ˆå¦‚ä½¿ç”¨ Gitï¼‰
   ```bash
   git checkout -b feature/new-feature
   ```

2. **ä¿®æ”¹ä»£ç **
   - éµå¾ªç°æœ‰ä»£ç é£æ ¼
   - ä½¿ç”¨ Pydantic æ¨¡å‹éªŒè¯æ•°æ®
   - å¼‚æ­¥å‡½æ•°ä½¿ç”¨ `async/await`
   - æ·»åŠ æ—¥å¿—è®°å½•

3. **ä»£ç æ£€æŸ¥**
   ```bash
   ./lint.sh
   ```

4. **æµ‹è¯•**
   ```bash
   # Web ç•Œé¢æµ‹è¯•
   ./run_web.sh

   # ç‹¬ç«‹æ”¶é›†å™¨æµ‹è¯•
   ./run_xhs_scraper.sh "æµ‹è¯•æ™¯ç‚¹" -n 3
   ```

5. **æ¸…ç†ç¼“å­˜**ï¼ˆå¦‚æœ‰é—®é¢˜ï¼‰
   ```bash
   find . -name "*.pyc" -delete
   find . -name "__pycache__" -delete
   ```

### ä¿®æ”¹æ•°æ®æ¨¡å‹

**ä¿®æ”¹ TripPlan æˆ– Attraction æ—¶**:
1. æ•°æ®å­˜å‚¨åœ¨ `context` å­—å…¸ä¸­ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å®šä¹‰
2. ä½¿ç”¨ `set_context()` å†™å…¥ï¼Œ`get()` è¯»å–
3. ç¡®ä¿ç±»å‹åŒ¹é… Pydantic å®šä¹‰

**ç¤ºä¾‹**:
```python
# å†™å…¥æ•°æ®
trip_plan.set_ai_planning({
    "itinerary": {
        "day1": {...}
    }
})

# è¯»å–æ•°æ®
itinerary = trip_plan.get("ai_planning.itinerary", {})
```

### æ·»åŠ æ–°çˆ¬è™«

1. **ç»§æ‰¿ BrowserUseScraper**
   ```python
   from app.scrapers.browser_use_scraper import BrowserUseScraper

   class NewScraper(BrowserUseScraper):
       async def scrape(self, query: str):
           task = f"åœ¨XXXç½‘ç«™æœç´¢{query}..."
           result = await self.scrape_with_task(
               task=task,
               output_model=YourOutputModel,
               max_steps=20
           )
           return self._parse(result)
   ```

2. **åˆ›å»ºç‹¬ç«‹è¿è¡Œè„šæœ¬**
   ```python
   # app/scrapers/run_new.py
   async def run_new_scraper(query: str):
       scraper = NewScraper(headless=True)
       try:
           result = await scraper.scrape(query)
           print(json.dumps(result, ensure_ascii=False, indent=2))
       finally:
           await scraper.close()

   if __name__ == "__main__":
       asyncio.run(run_new_scraper(sys.argv[1]))
   ```

3. **åœ¨ PlannerAgent ä¸­ä½¿ç”¨**
   ```python
   new_scraper = NewScraper(headless=self.headless)
   data = await new_scraper.scrape(query)
   await new_scraper.close()  # è®°å¾—å…³é—­æµè§ˆå™¨
   ```

## ğŸ“ æ¶æ„å†³ç­–è®°å½•

### ä¸ºä»€ä¹ˆä½¿ç”¨ä¸Šä¸‹æ–‡æ•°æ®æ¨¡å‹ï¼Ÿ

**ä¼ ç»Ÿå›ºå®šå±æ€§æ–¹å¼çš„é—®é¢˜**:
- AI ç”Ÿæˆçš„æ•°æ®ç»“æ„éš¾ä»¥é¢„æµ‹
- é¢‘ç¹éœ€è¦ä¿®æ”¹æ¨¡å‹å®šä¹‰
- ä¸åŒæ•°æ®æºæœ‰ä¸åŒå­—æ®µ

**ä¸Šä¸‹æ–‡æ–¹å¼çš„ä¼˜åŠ¿**:
- âœ… çµæ´»å­˜å‚¨ä»»æ„ç»“æ„æ•°æ®
- âœ… AI å¯è‡ªç”±å¡«å……å†…å®¹
- âœ… æ‰©å±•æ€§å¼ºï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

### ä¸ºä»€ä¹ˆä½¿ç”¨ Browser-Useï¼Ÿ

**ç›¸æ¯”ä¼ ç»Ÿçˆ¬è™«ï¼ˆBeautifulSoup/Seleniumï¼‰**:
- âœ… AI è‡ªåŠ¨å¤„ç†é¡µé¢å˜åŒ–
- âœ… è‡ªç„¶è¯­è¨€æè¿°ä»»åŠ¡
- âœ… æ— éœ€ç»´æŠ¤é€‰æ‹©å™¨
- âœ… è‡ªåŠ¨å¤„ç†äº¤äº’ï¼ˆç‚¹å‡»ã€æ»šåŠ¨ï¼‰
- âœ… æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼ˆPydanticï¼‰

### ä¸ºä»€ä¹ˆé€‰æ‹© Streamlitï¼Ÿ

**Streamlit å•ä½“æ¶æ„çš„ä¼˜åŠ¿**:
- âœ… å¿«é€Ÿæ„å»º Web UI
- âœ… Python åŸç”Ÿï¼Œæ— éœ€å‰åç«¯åˆ†ç¦»
- âœ… è‡ªåŠ¨å¤„ç†çŠ¶æ€ç®¡ç†
- âœ… é€‚åˆ AI åº”ç”¨åŸå‹
- âœ… å‰åç«¯ç›´æ¥å‡½æ•°è°ƒç”¨ï¼Œæ€§èƒ½é«˜

**å±€é™æ€§**:
- âŒ è€¦åˆåº¦é«˜ï¼Œéš¾ä»¥ç‹¬ç«‹æ‰©å±•
- âŒ å•ç‚¹æ•…éšœ
- âŒ èµ„æºå…±äº«ï¼ˆæµè§ˆå™¨å’ŒæœåŠ¡å™¨åœ¨åŒä¸€è¿›ç¨‹ï¼‰

**æœªæ¥æ‰©å±•æ–¹å‘**:
- å°†åç«¯æ”¹ä¸º FastAPI REST API
- å‰ç«¯é€šè¿‡ HTTP è°ƒç”¨
- ä½¿ç”¨æ¶ˆæ¯é˜Ÿåˆ—å¤„ç†å¼‚æ­¥ä»»åŠ¡
- ç‹¬ç«‹éƒ¨ç½²æ”¶é›†å™¨æœåŠ¡

### ä¸ºä»€ä¹ˆè®¾è®¡ç‹¬ç«‹æ”¶é›†å™¨ï¼Ÿ

**åŠ¨æœº**:
- ä¾¿äºè°ƒè¯•å’Œæµ‹è¯•
- æ”¯æŒå‘½ä»¤è¡Œæ‰¹é‡å¤„ç†
- å¯ä»¥é›†æˆåˆ°å…¶ä»–ç³»ç»Ÿ
- è§£è€¦æ•°æ®æ”¶é›†å’Œä¸šåŠ¡é€»è¾‘

**å®ç°** (app/scrapers/run_xhs.py, run_official.py):
- æ ‡å‡†åŒ–çš„å‘½ä»¤è¡Œæ¥å£
- JSON æ ¼å¼è¾“å‡º
- å®Œæ•´çš„æ—¥å¿—è¿½è¸ª
- é”™è¯¯å¤„ç†å’Œä¼˜é›…é™çº§

## ğŸ“š ç›¸å…³èµ„æº

- [Browser-Use æ–‡æ¡£](https://github.com/browser-use/browser-use)
- [Streamlit æ–‡æ¡£](https://docs.streamlit.io)
- [Pydantic æ–‡æ¡£](https://docs.pydantic.dev)
- [Pylint æ–‡æ¡£](https://pylint.readthedocs.io)
- [AsyncIO æ–‡æ¡£](https://docs.python.org/3/library/asyncio.html)

## ğŸ“‹ ä»£ç å‚è€ƒç´¢å¼•

**æ ¸å¿ƒæ–‡ä»¶ä½ç½®**:
- å‰ç«¯å…¥å£: `frontend/app.py:283` (å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ)
- åç«¯æ ¸å¿ƒ: `app/agents/planner_agent.py:36` (æ—…è¡Œè§„åˆ’é€»è¾‘)
- AI çˆ¬è™«åŸºç±»: `app/scrapers/browser_use_scraper.py:215` (Browser-Use é›†æˆ)
- å°çº¢ä¹¦çˆ¬è™«: `app/scrapers/xhs_scraper.py:34` (æœç´¢ç¬”è®°)
- å®˜ç½‘çˆ¬è™«: `app/scrapers/official_scraper.py:26` (æå–å®˜ç½‘ä¿¡æ¯)
- æ•°æ®æ¨¡å‹: `app/models/attraction.py:46` (XHSNote å®šä¹‰)
- æ—¥å¿—å·¥å…·: `app/utils/logger.py` (ç»“æ„åŒ–æ—¥å¿—)

---

**æœ€åæ›´æ–°**: 2025-10-07
**ç»´æŠ¤è€…**: Browser-Brain Team
**ç‰ˆæœ¬**: v1.3 (æ–°å¢éªŒè¯ç äººå·¥å¤„ç†ã€å¢å¼ºåæ£€æµ‹é…ç½®)
