"""æµ‹è¯•Browser-Useçˆ¬è™« - æœ‰å¤´æµè§ˆå™¨æ¨¡å¼"""
import asyncio
from app.scrapers.xhs_scraper import XHSScraper
from app.scrapers.official_scraper import OfficialScraper
from app.utils.logger import setup_logger

logger = setup_logger(__name__)


async def test_xhs_scraper():
    """æµ‹è¯•å°çº¢ä¹¦çˆ¬è™« - æ˜¾ç¤ºæµè§ˆå™¨çª—å£"""
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æµ‹è¯•å°çº¢ä¹¦çˆ¬è™« (æœ‰å¤´æµè§ˆå™¨æ¨¡å¼)")
    print("=" * 60)

    scraper = XHSScraper(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£!

    try:
        print("\nğŸ“Œ æ­£åœ¨æœç´¢: æ•…å®«")
        print("ğŸ’¡ ä½ åº”è¯¥èƒ½çœ‹åˆ°Chromeçª—å£æ‰“å¼€å¹¶è‡ªåŠ¨æ“ä½œ...\n")

        notes = await scraper.search_attraction("æ•…å®«", max_notes=3)

        print(f"\nâœ… çˆ¬å–å®Œæˆ! è·å¾— {len(notes)} ç¯‡ç¬”è®°")

        if notes:
            for i, note in enumerate(notes, 1):
                print(f"\nğŸ“ ç¬”è®° {i}:")
                print(f"  æ ‡é¢˜: {note.title}")
                print(f"  ä½œè€…: {note.author}")
                print(f"  ç‚¹èµ: {note.likes}, æ”¶è—: {note.collects}")
                print(f"  å†…å®¹é¢„è§ˆ: {note.content[:100]}...")
        else:
            print("âš ï¸  æœªè·å–åˆ°ç¬”è®°æ•°æ®")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.close()
        print("\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")


async def test_official_scraper():
    """æµ‹è¯•å®˜ç½‘çˆ¬è™« - æ˜¾ç¤ºæµè§ˆå™¨çª—å£"""
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æµ‹è¯•å®˜ç½‘çˆ¬è™« (æœ‰å¤´æµè§ˆå™¨æ¨¡å¼)")
    print("=" * 60)

    scraper = OfficialScraper(headless=False)  # æ˜¾ç¤ºæµè§ˆå™¨çª—å£!

    try:
        print("\nğŸ“Œ æ­£åœ¨æœç´¢: æ•…å®«å®˜ç½‘")
        print("ğŸ’¡ ä½ åº”è¯¥èƒ½çœ‹åˆ°Chromeçª—å£æ‰“å¼€å¹¶è‡ªåŠ¨æ“ä½œ...\n")

        info = await scraper.get_official_info("æ•…å®«")

        if info:
            print(f"\nâœ… çˆ¬å–å®Œæˆ!")
            print(f"  å®˜ç½‘: {info.website}")
            print(f"  å¼€æ”¾æ—¶é—´: {info.opening_hours}")
            print(f"  é—¨ç¥¨ä»·æ ¼: {info.ticket_price}")
            print(f"  åœ°å€: {info.address}")
        else:
            print("âš ï¸  æœªè·å–åˆ°å®˜ç½‘ä¿¡æ¯")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await scraper.close()
        print("\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")


async def main():
    print("\nğŸ¯ Browser-Use çˆ¬è™«æµ‹è¯•")
    print("=" * 60)
    print("é…ç½®:")
    print("  - headless=False (æ˜¾ç¤ºæµè§ˆå™¨)")
    print("  - wait_between_actions=1.0 (æ¨¡æ‹Ÿäººç±»é€Ÿåº¦)")
    print("  - use_vision=True (AIè§†è§‰èƒ½åŠ›)")
    print("=" * 60)

    # æµ‹è¯•å°çº¢ä¹¦çˆ¬è™«
    await test_xhs_scraper()

    # ç­‰å¾…ä¸€ä¸‹,é¿å…APIé…é¢é—®é¢˜
    print("\nâ³ ç­‰å¾…60ç§’,é¿å…APIé…é¢é™åˆ¶...")
    await asyncio.sleep(60)

    # æµ‹è¯•å®˜ç½‘çˆ¬è™«
    await test_official_scraper()

    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
