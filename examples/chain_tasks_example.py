#!/usr/bin/env python3
"""
Browser-Use Chain Agent Tasks ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Keep-Aliveæ¨¡å¼å’Œä»»åŠ¡é“¾å¼æ‰§è¡ŒåŠŸèƒ½
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.scrapers.xhs_scraper import XHSScraper
from app.utils.logger import setup_logger

logger = setup_logger(__name__)


async def example_1_basic_chain():
    """
    ç¤ºä¾‹1: åŸºç¡€ä»»åŠ¡é“¾æ‰§è¡Œ

    åœºæ™¯: åœ¨å°çº¢ä¹¦ä¸Šæœç´¢å¹¶ç‚¹å‡»ç¬¬ä¸€ä¸ªç»“æœ
    """
    logger.info("=" * 60)
    logger.info("ç¤ºä¾‹1: åŸºç¡€ä»»åŠ¡é“¾æ‰§è¡Œ")
    logger.info("=" * 60)

    # å¯ç”¨Keep-Aliveæ¨¡å¼
    scraper = XHSScraper(headless=False, keep_alive=True)

    try:
        # å®šä¹‰ä»»åŠ¡é“¾
        tasks = [
            "è®¿é—®å°çº¢ä¹¦ç½‘ç«™ https://www.xiaohongshu.com",
            "åœ¨æœç´¢æ¡†è¾“å…¥'åŒ—äº¬æ•…å®«'å¹¶æœç´¢",
            "æå–ç¬¬ä¸€ä¸ªæœç´¢ç»“æœçš„æ ‡é¢˜"
        ]

        # æ‰§è¡Œä»»åŠ¡é“¾
        results = await scraper.run_task_chain(tasks, max_steps_per_task=10)

        # æ‰“å°ç»“æœ
        for result in results:
            logger.info(f"ä»»åŠ¡{result['task_index']}: {result['status']}")
            if result['status'] == 'success':
                logger.info(f"ç»“æœ: {result['data']}")

    finally:
        # Keep-Aliveæ¨¡å¼ï¼šéœ€è¦æ‰‹åŠ¨å¼ºåˆ¶å…³é—­
        await scraper.close(force=True)


async def example_2_multi_step_extraction():
    """
    ç¤ºä¾‹2: å¤šæ­¥æ•°æ®æå–

    åœºæ™¯: æœç´¢ â†’ ç‚¹å‡» â†’ æå–è¯¦ç»†ä¿¡æ¯
    """
    logger.info("=" * 60)
    logger.info("ç¤ºä¾‹2: å¤šæ­¥æ•°æ®æå–")
    logger.info("=" * 60)

    scraper = XHSScraper(
        headless=False,
        keep_alive=True,
        fast_mode=True  # å¯ç”¨Fast ModeåŠ é€Ÿ
    )

    try:
        tasks = [
            "è®¿é—®å°çº¢ä¹¦å¹¶æœç´¢'åŒ—äº¬æ—…æ¸¸æ”»ç•¥'",
            "æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šç»“æœ",
            "ç‚¹å‡»ç¬¬ä¸€ä¸ªç¬”è®°è¿›å…¥è¯¦æƒ…é¡µ",
            "æå–ç¬”è®°æ ‡é¢˜ã€ä½œè€…ã€ç‚¹èµæ•°å’Œå†…å®¹æ‘˜è¦"
        ]

        results = await scraper.run_task_chain(tasks, max_steps_per_task=15)

        # æå–æœ€åä¸€æ­¥çš„æ•°æ®
        if results and results[-1]['status'] == 'success':
            final_data = results[-1]['data']
            logger.info(f"âœ… æå–æˆåŠŸ: {final_data}")
        else:
            logger.error("âŒ ä»»åŠ¡é“¾æ‰§è¡Œå¤±è´¥")

    finally:
        await scraper.close(force=True)


async def example_3_conversational_flow():
    """
    ç¤ºä¾‹3: å¯¹è¯å¼äº¤äº’æµç¨‹

    åœºæ™¯: æ¨¡æ‹Ÿç”¨æˆ·å¯¹è¯å¼æŸ¥è¯¢
    """
    logger.info("=" * 60)
    logger.info("ç¤ºä¾‹3: å¯¹è¯å¼äº¤äº’æµç¨‹")
    logger.info("=" * 60)

    scraper = XHSScraper(headless=False, keep_alive=True)

    try:
        # ç¬¬ä¸€è½®å¯¹è¯
        logger.info("ç”¨æˆ·: æœç´¢åŒ—äº¬æ—…æ¸¸ç›¸å…³çš„ç¬”è®°")
        task1_result = await scraper.run_task_chain([
            "åœ¨å°çº¢ä¹¦æœç´¢'åŒ—äº¬æ—…æ¸¸'"
        ])

        # ç¬¬äºŒè½®å¯¹è¯ï¼ˆåŸºäºä¸Šä¸€è½®çš„ç»“æœï¼‰
        logger.info("ç”¨æˆ·: å‘Šè¯‰æˆ‘ç¬¬ä¸€ä¸ªç»“æœæ˜¯ä»€ä¹ˆ")
        task2_result = await scraper.run_task_chain([
            "æå–ç¬¬ä¸€ä¸ªæœç´¢ç»“æœçš„æ ‡é¢˜å’Œä½œè€…"
        ])

        # ç¬¬ä¸‰è½®å¯¹è¯
        logger.info("ç”¨æˆ·: ç‚¹å‡»è¿›å»çœ‹çœ‹è¯¦ç»†å†…å®¹")
        task3_result = await scraper.run_task_chain([
            "ç‚¹å‡»ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ",
            "æå–ç¬”è®°çš„å®Œæ•´å†…å®¹"
        ])

        logger.info("âœ… å¯¹è¯å¼äº¤äº’å®Œæˆ")

    finally:
        await scraper.close(force=True)


async def example_4_error_handling():
    """
    ç¤ºä¾‹4: é”™è¯¯å¤„ç†å’Œé™çº§

    åœºæ™¯: ä»»åŠ¡é“¾ä¸­æŸä¸€æ­¥å¤±è´¥åçš„å¤„ç†
    """
    logger.info("=" * 60)
    logger.info("ç¤ºä¾‹4: é”™è¯¯å¤„ç†å’Œé™çº§")
    logger.info("=" * 60)

    scraper = XHSScraper(headless=False, keep_alive=True)

    try:
        # æ•…æ„åŒ…å«ä¸€ä¸ªå¯èƒ½å¤±è´¥çš„ä»»åŠ¡
        tasks = [
            "è®¿é—®å°çº¢ä¹¦ç½‘ç«™",
            "æœç´¢'åŒ—äº¬æ•…å®«'",
            "ç‚¹å‡»ä¸å­˜åœ¨çš„å…ƒç´ ",  # è¿™ä¸€æ­¥å¯èƒ½å¤±è´¥
            "æå–ç»“æœ"  # è¿™ä¸€æ­¥ä¸ä¼šæ‰§è¡Œ
        ]

        results = await scraper.run_task_chain(tasks, max_steps_per_task=10)

        # åˆ†æç»“æœ
        success_count = sum(1 for r in results if r['status'] == 'success')
        logger.info(f"âœ… æˆåŠŸ: {success_count}/{len(results)} ä¸ªä»»åŠ¡")

        # æ‰¾åˆ°å¤±è´¥çš„ä»»åŠ¡
        for result in results:
            if result['status'] != 'success':
                logger.error(
                    f"âŒ ä»»åŠ¡{result['task_index']}å¤±è´¥: "
                    f"{result.get('error', 'Unknown error')}"
                )

    finally:
        await scraper.close(force=True)


async def example_5_keep_alive_comparison():
    """
    ç¤ºä¾‹5: Keep-Aliveæ¨¡å¼æ€§èƒ½å¯¹æ¯”

    å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨Keep-Aliveçš„æ€§èƒ½å·®å¼‚
    """
    logger.info("=" * 60)
    logger.info("ç¤ºä¾‹5: Keep-Aliveæ¨¡å¼æ€§èƒ½å¯¹æ¯”")
    logger.info("=" * 60)

    import time

    tasks = [
        "è®¿é—®å°çº¢ä¹¦",
        "æœç´¢'æµ‹è¯•'",
        "æå–ç¬¬ä¸€ä¸ªç»“æœ"
    ]

    # æ–¹æ¡ˆ1: ä¸ä½¿ç”¨Keep-Aliveï¼ˆæ¯ä¸ªä»»åŠ¡é‡æ–°å¯åŠ¨æµè§ˆå™¨ï¼‰
    logger.info("ğŸ“Š æµ‹è¯•æ–¹æ¡ˆ1: æ ‡å‡†æ¨¡å¼ï¼ˆæ¯æ¬¡é‡æ–°å¯åŠ¨æµè§ˆå™¨ï¼‰")
    start = time.time()

    for task in tasks:
        scraper = XHSScraper(headless=True, keep_alive=False)
        try:
            result = await scraper.run_task_chain([task], max_steps_per_task=5)
        finally:
            await scraper.close()

    standard_time = time.time() - start
    logger.info(f"æ ‡å‡†æ¨¡å¼è€—æ—¶: {standard_time:.1f}ç§’")

    # æ–¹æ¡ˆ2: ä½¿ç”¨Keep-Aliveï¼ˆæµè§ˆå™¨ä¿æŒæ´»è·ƒï¼‰
    logger.info("ğŸ“Š æµ‹è¯•æ–¹æ¡ˆ2: Keep-Aliveæ¨¡å¼")
    start = time.time()

    scraper = XHSScraper(headless=True, keep_alive=True)
    try:
        result = await scraper.run_task_chain(tasks, max_steps_per_task=5)
    finally:
        await scraper.close(force=True)

    keepalive_time = time.time() - start
    logger.info(f"Keep-Aliveæ¨¡å¼è€—æ—¶: {keepalive_time:.1f}ç§’")

    # å¯¹æ¯”
    speedup = standard_time / keepalive_time if keepalive_time > 0 else 0
    logger.info(f"ğŸš€ é€Ÿåº¦æå‡: {speedup:.1f}x")


async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    examples = [
        ("åŸºç¡€ä»»åŠ¡é“¾", example_1_basic_chain),
        ("å¤šæ­¥æ•°æ®æå–", example_2_multi_step_extraction),
        ("å¯¹è¯å¼äº¤äº’", example_3_conversational_flow),
        ("é”™è¯¯å¤„ç†", example_4_error_handling),
        ("æ€§èƒ½å¯¹æ¯”", example_5_keep_alive_comparison),
    ]

    logger.info("ğŸ¯ Browser-Use Chain Agent Tasks ç¤ºä¾‹é›†")
    logger.info(f"å…± {len(examples)} ä¸ªç¤ºä¾‹\n")

    for idx, (name, func) in enumerate(examples, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"è¿è¡Œç¤ºä¾‹ {idx}/{len(examples)}: {name}")
        logger.info(f"{'='*60}\n")

        try:
            await func()
            logger.info(f"âœ… ç¤ºä¾‹ {idx} å®Œæˆ\n")
        except Exception as e:
            logger.exception(f"âŒ ç¤ºä¾‹ {idx} å¤±è´¥: {e}\n")

        # ç¤ºä¾‹é—´é—´éš”
        await asyncio.sleep(2)

    logger.info("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ")


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    asyncio.run(main())

    # æˆ–è€…å•ç‹¬è¿è¡ŒæŸä¸ªç¤ºä¾‹
    # asyncio.run(example_1_basic_chain())
