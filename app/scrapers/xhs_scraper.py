"""åŸºäºBrowser-Useçš„å°çº¢ä¹¦AIçˆ¬è™«"""
import asyncio
from typing import List
from pydantic import BaseModel, Field
from app.scrapers.browser_use_scraper import BrowserUseScraper
from app.models.attraction import XHSNote
from app.utils.logger import setup_logger, log_function_call, log_step
from datetime import datetime

logger = setup_logger(__name__)


# å®šä¹‰å°çº¢ä¹¦ç¬”è®°çš„ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
class XHSNoteOutput(BaseModel):
    """å•æ¡å°çº¢ä¹¦ç¬”è®°è¾“å‡º"""
    title: str = Field(description="ç¬”è®°æ ‡é¢˜")
    author: str = Field(description="ä½œè€…åç§°")
    content: str = Field(description="ç¬”è®°æ­£æ–‡å†…å®¹")
    likes: int = Field(default=0, description="ç‚¹èµæ•°")
    collects: int = Field(default=0, description="æ”¶è—æ•°")
    comments: int = Field(default=0, description="è¯„è®ºæ•°")
    extracted_links: List[str] = Field(default_factory=list, description="æå–çš„URLé“¾æ¥ï¼ˆå®˜ç½‘ã€é¢„è®¢é“¾æ¥ç­‰ï¼‰")
    keywords: List[str] = Field(default_factory=list, description="å…³é”®è¯ï¼ˆå®˜ç½‘ã€é¢„è®¢ã€é—¨ç¥¨ç­‰ï¼‰")
    images: List[str] = Field(default_factory=list, description="å›¾ç‰‡URLåˆ—è¡¨")


class XHSNotesCollection(BaseModel):
    """å°çº¢ä¹¦ç¬”è®°é›†åˆ"""
    notes: List[XHSNoteOutput] = Field(description="ç¬”è®°åˆ—è¡¨")


class XHSScraper(BrowserUseScraper):
    """åŸºäºBrowser-Useçš„å°çº¢ä¹¦AIçˆ¬è™«"""

    async def _handle_captcha_manual(self, wait_seconds: int = 60):
        """
        éªŒè¯ç äººå·¥å¤„ç†ï¼šæš‚åœç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯

        Args:
            wait_seconds: ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’
        """
        logger.warning("âš ï¸  æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæš‚åœç­‰å¾…äººå·¥å¤„ç†...")
        logger.info("ğŸ“Œ è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®ŒæˆéªŒè¯ç éªŒè¯")
        logger.info(f"â³ ç³»ç»Ÿå°†åœ¨ {wait_seconds} ç§’åè‡ªåŠ¨ç»§ç»­...")

        # æ¯10ç§’æç¤ºä¸€æ¬¡å‰©ä½™æ—¶é—´
        for remaining in range(wait_seconds, 0, -10):
            logger.info(f"â±ï¸  å‰©ä½™ç­‰å¾…æ—¶é—´: {remaining} ç§’")
            await asyncio.sleep(min(10, remaining))

        logger.info("âœ… ç­‰å¾…ç»“æŸï¼Œç»§ç»­æ‰§è¡Œä»»åŠ¡...")

    @log_function_call
    async def search_attraction(self, attraction_name: str, max_notes: int = 5) -> List[XHSNote]:
        """
        ä½¿ç”¨AIæœç´¢æ™¯ç‚¹ç›¸å…³ç¬”è®°

        Args:
            attraction_name: æ™¯ç‚¹åç§°
            max_notes: æœ€å¤§ç¬”è®°æ•°é‡

        Returns:
            å°çº¢ä¹¦ç¬”è®°åˆ—è¡¨
        """
        logger.info(f"========== å¼€å§‹å°çº¢ä¹¦çˆ¬å– ==========")
        logger.info(f"ç›®æ ‡æ™¯ç‚¹: {attraction_name}, ç›®æ ‡ç¬”è®°æ•°: {max_notes}")
        log_step(1, "å‡†å¤‡å°çº¢ä¹¦æœç´¢ä»»åŠ¡", attraction=attraction_name, max_notes=max_notes)

        # æ„å»ºAIä»»åŠ¡æè¿°
        task = f"""
ä»»åŠ¡ï¼šåœ¨å°çº¢ä¹¦æœç´¢"{attraction_name}"ç›¸å…³çš„æ—…æ¸¸ç¬”è®°

å…·ä½“æ­¥éª¤ï¼š
1. è®¿é—®å°çº¢ä¹¦ç½‘ç«™ https://www.xiaohongshu.com
2. ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½(3-5ç§’)
3. åœ¨æœç´¢æ¡†ä¸­è¾“å…¥å…³é”®è¯ï¼š"{attraction_name}"
4. ç‚¹å‡»æœç´¢æˆ–æŒ‰å›è½¦é”®
5. ç­‰å¾…æœç´¢ç»“æœåŠ è½½å®Œæˆ
6. æµè§ˆæœç´¢ç»“æœï¼Œæ‰¾åˆ°å‰{max_notes}ç¯‡ç›¸å…³ç¬”è®°
7. å¯¹äºæ¯ç¯‡ç¬”è®°ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
   - ç¬”è®°æ ‡é¢˜
   - ä½œè€…åç§°
   - ç¬”è®°æ­£æ–‡å†…å®¹ï¼ˆå°½å¯èƒ½å®Œæ•´ï¼‰
   - ç‚¹èµæ•°ã€æ”¶è—æ•°ã€è¯„è®ºæ•°
   - ç¬”è®°ä¸­çš„å›¾ç‰‡URLï¼ˆå‰3å¼ ï¼‰
   - æå–ç¬”è®°ä¸­æåˆ°çš„URLé“¾æ¥ï¼ˆç‰¹åˆ«æ˜¯å®˜ç½‘ã€é¢„è®¢ã€é—¨ç¥¨ç›¸å…³é“¾æ¥ï¼‰
   - è¯†åˆ«å…³é”®è¯ï¼ˆå¦‚ï¼šå®˜ç½‘ã€å®˜æ–¹ç½‘ç«™ã€é¢„è®¢ã€é—¨ç¥¨ã€å¼€æ”¾æ—¶é—´ç­‰ï¼‰

é‡è¦æç¤ºï¼š
- åƒçœŸå®ç”¨æˆ·ä¸€æ ·æ“ä½œï¼Œæ¯æ­¥ä¹‹é—´ç•™æœ‰é—´éš”
- ä¼˜å…ˆé€‰æ‹©ç‚¹èµæ•°å’Œæ”¶è—æ•°è¾ƒé«˜çš„ç¬”è®°
- å¦‚æœé‡åˆ°ä»»ä½•å¼¹çª—æˆ–å¼•å¯¼ï¼Œå…ˆå…³é—­å®ƒä»¬
- å¦‚æœé‡åˆ°ç™»å½•è¦æ±‚ï¼Œç‚¹å‡»"ç¨åå†è¯´"æˆ–å…³é—­æŒ‰é’®
- é‡åˆ°éªŒè¯ç æ—¶ï¼Œæš‚åœå¹¶ç­‰å¾…äººå·¥å¤„ç†
- è¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®
"""

        # ä½¿ç”¨AIæ‰§è¡Œçˆ¬å–
        log_step(2, "è°ƒç”¨Browser-Use AIæ‰§è¡Œå°çº¢ä¹¦çˆ¬å–")
        result = await self.scrape_with_task(
            task=task,
            output_model=XHSNotesCollection,
            max_steps=30  # å°çº¢ä¹¦éœ€è¦å¤šæ­¥æ“ä½œ
        )

        # æ£€æµ‹æ˜¯å¦é‡åˆ°éªŒè¯ç 
        if result["status"] == "success" and result.get("urls"):
            visited_urls = result["urls"]
            # æ£€æŸ¥æ˜¯å¦è®¿é—®äº†éªŒè¯ç é¡µé¢
            if any("captcha" in url.lower() for url in visited_urls):
                logger.warning("ğŸš« æ£€æµ‹åˆ°è®¿é—®äº†éªŒè¯ç é¡µé¢ï¼Œå¯åŠ¨äººå·¥å¤„ç†æµç¨‹...")

                # æš‚åœç­‰å¾…äººå·¥å¤„ç†
                await self._handle_captcha_manual(wait_seconds=60)

                # é‡æ–°å°è¯•æ‰§è¡Œä»»åŠ¡
                logger.info("ğŸ”„ é‡æ–°å°è¯•æ‰§è¡Œçˆ¬å–ä»»åŠ¡...")
                result = await self.scrape_with_task(
                    task=task,
                    output_model=XHSNotesCollection,
                    max_steps=30
                )

        log_step(3, "å¤„ç†AIè¿”å›ç»“æœ", status=result["status"])

        if result["status"] != "success":
            logger.error(f"âŒ AIçˆ¬å–å°çº¢ä¹¦å¤±è´¥: {result.get('error', 'Unknown error')}")
            logger.error(f"æ‰§è¡Œæ­¥éª¤æ•°: {result.get('steps', 0)}, è®¿é—®çš„URL: {result.get('urls', [])}")
            return []

        # è½¬æ¢ä¸ºXHSNoteå¯¹è±¡
        notes_data = result["data"]
        xhs_notes = []

        # å¤„ç† AI Agent å¤±è´¥è¿”å›å­—ç¬¦ä¸²æˆ–æ— æ•°æ®çš„æƒ…å†µ
        if isinstance(notes_data, str) or not hasattr(notes_data, 'notes'):
            logger.warning(f"âš ï¸  AIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸æˆ–æ— æ•°æ®: {type(notes_data)}")
            logger.debug(f"åŸå§‹è¿”å›æ•°æ®: {notes_data}")
            return []

        logger.info(f"AIæˆåŠŸè¿”å› {len(notes_data.notes)} ç¯‡ç¬”è®°æ•°æ®")
        log_step(4, "è½¬æ¢ç¬”è®°æ•°æ®ä¸ºXHSNoteå¯¹è±¡", note_count=len(notes_data.notes))

        for idx, note_output in enumerate(notes_data.notes):
            note = XHSNote(
                note_id=f"xhs_{attraction_name}_{idx}",
                title=note_output.title,
                author=note_output.author,
                content=note_output.content,
                likes=note_output.likes,
                collects=note_output.collects,
                comments=note_output.comments,
                images=note_output.images[:5],  # æœ€å¤š5å¼ å›¾ç‰‡
                extracted_links=note_output.extracted_links,
                keywords=note_output.keywords,
                created_at=datetime.now()
            )
            xhs_notes.append(note)
            logger.debug(f"ç¬”è®° {idx+1}: {note.title[:30]}... (ç‚¹èµ:{note.likes}, æ”¶è—:{note.collects})")

        logger.info(f"âœ… æˆåŠŸçˆ¬å– {len(xhs_notes)} ç¯‡å°çº¢ä¹¦ç¬”è®°")
        logger.info(f"========== å°çº¢ä¹¦çˆ¬å–å®Œæˆ ==========")
        return xhs_notes

    async def scrape(self, attraction_name: str, max_notes: int = 10) -> List[XHSNote]:
        """
        å®ç°åŸºç±»çš„æŠ½è±¡æ–¹æ³•

        Args:
            attraction_name: æ™¯ç‚¹åç§°
            max_notes: æœ€å¤§ç¬”è®°æ•°

        Returns:
            ç¬”è®°åˆ—è¡¨
        """
        return await self.search_attraction(attraction_name, max_notes)
