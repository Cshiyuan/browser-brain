"""åŸºäºBrowser-Useçš„AIé©±åŠ¨çˆ¬è™«åŸºç±»"""
import asyncio
from typing import Optional, List
from pathlib import Path
from browser_use import Agent, BrowserSession, BrowserProfile, ChatGoogle
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from pydantic import BaseModel
from app.utils.logger import (
    setup_logger,
    setup_browser_use_logging,
    log_function_call,
    log_step,
    log_api_call
)
from config.settings import settings

logger = setup_logger(__name__)

# Fast Agenté€Ÿåº¦ä¼˜åŒ–æç¤ºè¯
SPEED_OPTIMIZATION_PROMPT = """
Speed optimization instructions:
- Be extremely concise and direct in your responses
- Get to the goal as quickly as possible
- Use multi-action sequences whenever possible to reduce steps
- Minimize thinking time and focus on action execution
"""


class BrowserUseScraper:
    """Browser-Use AIé©±åŠ¨çš„çˆ¬è™«åŸºç±»"""

    def __init__(self, headless: bool = None, fast_mode: bool = False, keep_alive: bool = False):
        """
        åˆå§‹åŒ–Browser-Useçˆ¬è™«

        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼
            fast_mode: æ˜¯å¦å¯ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆä¼˜åŒ–é€Ÿåº¦ï¼‰
            keep_alive: æ˜¯å¦ä¿æŒæµè§ˆå™¨ä¼šè¯ï¼ˆç”¨äºä»»åŠ¡é“¾å¼æ‰§è¡Œï¼‰
        """
        self.headless = headless if headless is not None else settings.HEADLESS
        self.fast_mode = fast_mode
        self.keep_alive = keep_alive
        self.llm = self._initialize_llm()
        self.browser_session: Optional[BrowserSession] = None
        self.browser_profile = self._create_browser_profile()
        self.current_agent: Optional[Agent] = None  # ç”¨äºä»»åŠ¡é“¾å¼æ‰§è¡Œ

        # è®¾ç½® browser-use æ—¥å¿—æ•è·ï¼ˆç»Ÿä¸€æ—¥å¿—ç®¡ç†ï¼‰
        setup_browser_use_logging()

        if self.keep_alive:
            logger.info("ğŸ”— Keep-Aliveæ¨¡å¼å·²å¯ç”¨ï¼šæµè§ˆå™¨ä¼šè¯å°†ä¿æŒæ´»è·ƒ")

    def _initialize_llm(self):
        """åˆå§‹åŒ–LLM"""
        log_step(1, "åˆå§‹åŒ–LLMé…ç½®")
        provider = settings.LLM_PROVIDER.lower()
        model = settings.LLM_MODEL
        logger.info(f"LLMé…ç½®: provider={provider}, model={model}")

        if provider == "google":
            if not settings.GOOGLE_API_KEY:
                logger.error("GOOGLE_API_KEYæœªè®¾ç½®")
                raise ValueError("GOOGLE_API_KEY not set in environment")
            logger.info(f"âœ“ ä½¿ç”¨ Google Gemini (browser-use ChatGoogle): {model}")
            # ä½¿ç”¨ browser-use å†…ç½®çš„ ChatGoogle ç±»
            return ChatGoogle(
                model=model,
                api_key=settings.GOOGLE_API_KEY
            )
        elif provider == "anthropic":
            if not settings.ANTHROPIC_API_KEY:
                logger.error("ANTHROPIC_API_KEYæœªè®¾ç½®")
                raise ValueError("ANTHROPIC_API_KEY not set in environment")
            logger.info(f"âœ“ ä½¿ç”¨ Anthropic Claude: {model}")
            return ChatAnthropic(
                model=model,
                api_key=settings.ANTHROPIC_API_KEY
            )
        else:  # default to OpenAI
            if not settings.OPENAI_API_KEY:
                logger.error("OPENAI_API_KEYæœªè®¾ç½®")
                raise ValueError("OPENAI_API_KEY not set in environment")
            logger.info(f"âœ“ ä½¿ç”¨ OpenAI: {model}")
            return ChatOpenAI(
                model=model,
                api_key=settings.OPENAI_API_KEY
            )

    def _create_browser_profile(self) -> BrowserProfile:
        """åˆ›å»ºæ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„æµè§ˆå™¨é…ç½®ï¼ˆå¢å¼ºåæ£€æµ‹ï¼‰"""
        log_step(2, "åˆ›å»ºæµè§ˆå™¨é…ç½®", headless=self.headless)

        # åŸºç¡€åæ£€æµ‹å‚æ•°
        browser_args = [
            '--disable-blink-features=AutomationControlled',  # éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
            '--disable-dev-shm-usage',
            '--disable-infobars',  # éšè—è‡ªåŠ¨åŒ–ä¿¡æ¯æ 
        ]

        # æ ¹æ®æœ‰å¤´/æ— å¤´æ¨¡å¼æ·»åŠ ä¸åŒå‚æ•°
        if self.headless:
            # æ— å¤´æ¨¡å¼ï¼šæ·»åŠ å¿…è¦å‚æ•°
            browser_args.extend([
                '--no-sandbox',
                '--disable-gpu',
                '--window-size=1920,1080',  # è®¾ç½®çª—å£å¤§å°
            ])
            logger.info("æ— å¤´æ¨¡å¼: æ·»åŠ é¢å¤–æµè§ˆå™¨å‚æ•°")
        else:
            # æœ‰å¤´æ¨¡å¼ï¼šæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º
            browser_args.extend([
                '--start-maximized',  # æœ€å¤§åŒ–çª—å£ï¼ˆçœŸå®ç”¨æˆ·è¡Œä¸ºï¼‰
            ])
            logger.info("æœ‰å¤´æ¨¡å¼: æœ€å¤§åŒ–æµè§ˆå™¨çª—å£")

        logger.debug(f"æµè§ˆå™¨å‚æ•°: {browser_args}")

        # çœŸå®çš„ User-Agentï¼ˆMac Chromeï¼‰
        user_agent = (
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/120.0.0.0 Safari/537.36'
        )

        # Fast Modeä¼˜åŒ–ï¼šå‡å°‘ç­‰å¾…æ—¶é—´ä»¥æå‡é€Ÿåº¦
        if self.fast_mode:
            wait_page_load = 0.1
            max_page_load = 5.0
            wait_actions = 0.1
            logger.info("ğŸš€ Fast Modeå·²å¯ç”¨ï¼šæœ€å°åŒ–ç­‰å¾…æ—¶é—´")
        else:
            wait_page_load = 2.0
            max_page_load = 10.0
            wait_actions = 1.0
            logger.info("ğŸ¢ æ ‡å‡†æ¨¡å¼ï¼šæ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º")

        profile = BrowserProfile(
            headless=self.headless,
            disable_security=False,  # ä¿æŒå®‰å…¨ç‰¹æ€§,æ›´åƒçœŸå®æµè§ˆå™¨
            user_data_dir=None,
            args=browser_args,
            ignore_default_args=['--enable-automation'],  # éšè—è‡ªåŠ¨åŒ–æ ‡è¯†
            wait_for_network_idle_page_load_time=wait_page_load,  # Fast Mode: 0.1s, æ ‡å‡†: 2.0s
            maximum_wait_page_load_time=max_page_load,  # Fast Mode: 5.0s, æ ‡å‡†: 10.0s
            wait_between_actions=wait_actions,  # Fast Mode: 0.1s, æ ‡å‡†: 1.0s
        )

        mode_desc = "Fast Modeï¼ˆé€Ÿåº¦ä¼˜åŒ–ï¼‰" if self.fast_mode else "æ ‡å‡†æ¨¡å¼ï¼ˆåæ£€æµ‹ä¼˜åŒ–ï¼‰"
        logger.info(f"âœ“ æµè§ˆå™¨é…ç½®åˆ›å»ºå®Œæˆï¼ˆ{mode_desc}ï¼‰")
        return profile


    def _log_agent_steps(self, history):
        """è¯¦ç»†è®°å½• Agent æ‰§è¡Œçš„æ¯ä¸€æ­¥"""
        logger.info("=" * 60)
        logger.info("ğŸ” AI Agent æ‰§è¡Œæ­¥éª¤è¯¦æƒ…")
        logger.info("=" * 60)

        for i, step_history in enumerate(history.history, 1):
            logger.info(f"\nğŸ“ Step {i}/{len(history.history)}:")

            # è®°å½•è¯„ä¼°ç»“æœ
            if hasattr(step_history, 'model_output') and step_history.model_output:
                model_output = step_history.model_output

                # è·å–è¯„ä¼°ç»“æœï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬çš„ browser-useï¼‰
                evaluation = getattr(model_output, 'evaluation_previous_goal', None) or getattr(model_output, 'evaluation', None)
                if evaluation:
                    logger.info(f"   âš–ï¸  è¯„ä¼°: {evaluation}")

                    # ğŸ”´ æ£€æµ‹å…³é”®å¤±è´¥åŸå› 
                    eval_lower = str(evaluation).lower()
                    if 'security' in eval_lower or 'restriction' in eval_lower:
                        logger.error("   ğŸš« æ£€æµ‹åˆ°å®‰å…¨é™åˆ¶ï¼ˆåçˆ¬è™«æ‹¦æˆªï¼‰")
                    if 'captcha' in eval_lower:
                        logger.error("   ğŸš« æ£€æµ‹åˆ°éªŒè¯ç æŒ‘æˆ˜")
                    if 'failure' in eval_lower or 'failed' in eval_lower:
                        logger.warning("   âš ï¸  æ­¥éª¤æ‰§è¡Œå¤±è´¥")

                # è®°å½•ä¸‹ä¸€æ­¥ç›®æ ‡ï¼ˆç›´æ¥ä» Pydantic å¯¹è±¡è·å–å±æ€§ï¼‰
                next_goal = getattr(model_output, 'next_goal', None)
                if next_goal:
                    logger.info(f"   ğŸ¯ ä¸‹ä¸€æ­¥ç›®æ ‡: {next_goal}")

            # è®°å½•æ‰§è¡Œçš„åŠ¨ä½œ
            if hasattr(step_history, 'action_result') and step_history.action_result:
                actions = step_history.action_result
                if isinstance(actions, list):
                    for action in actions:
                        action_name = getattr(action, 'action_name', 'unknown')
                        logger.info(f"   ğŸ¦¾ æ‰§è¡ŒåŠ¨ä½œ: {action_name}")
                else:
                    logger.info(f"   ğŸ¦¾ æ‰§è¡ŒåŠ¨ä½œ: {actions}")

            # è®°å½•è®¿é—®çš„ URL
            if hasattr(step_history, 'state') and hasattr(step_history.state, 'url'):
                url = step_history.state.url
                logger.info(f"   ğŸ”— å½“å‰é¡µé¢: {url}")

        logger.info("\n" + "=" * 60)

        # æœ€ç»ˆç»“æœæ‘˜è¦
        final_result = history.final_result()
        if final_result:
            logger.info(f"ğŸ“„ æœ€ç»ˆç»“æœ: {final_result}")
        else:
            logger.warning("âš ï¸  æœªè·å–åˆ°æœ€ç»ˆç»“æœ")

        logger.info("=" * 60)

    async def _get_browser_session(self) -> BrowserSession:
        """è·å–æˆ–åˆ›å»ºBrowserSessionå®ä¾‹"""
        if self.browser_session is None:
            log_step(3, "åˆ›å»ºæµè§ˆå™¨ä¼šè¯", headless=self.headless)
            logger.info(f"æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            self.browser_session = BrowserSession(
                browser_profile=self.browser_profile
            )
            logger.info("âœ“ æµè§ˆå™¨ä¼šè¯åˆ›å»ºæˆåŠŸ")
        else:
            logger.debug("å¤ç”¨ç°æœ‰æµè§ˆå™¨ä¼šè¯")
        return self.browser_session

    async def scrape_with_task(
        self,
        task: str,
        output_model: Optional[type[BaseModel]] = None,
        max_steps: int = 20,
        use_vision: bool = True
    ) -> dict:
        """
        ä½¿ç”¨Browser-Use Agentæ‰§è¡Œçˆ¬å–ä»»åŠ¡

        Args:
            task: ä»»åŠ¡æè¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            output_model: Pydanticæ¨¡å‹ç±»ï¼Œç”¨äºç»“æ„åŒ–è¾“å‡º
            max_steps: æœ€å¤§æ­¥éª¤æ•°
            use_vision: æ˜¯å¦ä½¿ç”¨è§†è§‰èƒ½åŠ›ï¼ˆæˆªå›¾ç†è§£ï¼‰

        Returns:
            çˆ¬å–ç»“æœå­—å…¸
        """
        log_step(4, "å¼€å§‹AIçˆ¬å–ä»»åŠ¡", max_steps=max_steps, use_vision=use_vision)

        browser_session = await self._get_browser_session()

        logger.info(f"ä»»åŠ¡é…ç½®: max_steps={max_steps}, use_vision={use_vision}, output_model={output_model.__name__ if output_model else 'None'}")
        logger.debug(f"å®Œæ•´ä»»åŠ¡æè¿°:\n{task}")

        try:
            log_step(5, "åˆ›å»ºAI Agent")

            # Fast Modeä¼˜åŒ–ï¼šæ·»åŠ é€Ÿåº¦ä¼˜åŒ–æç¤ºè¯å’Œflash_mode
            agent_kwargs = {
                "task": task,
                "llm": self.llm,
                "browser_session": browser_session,
                "output_model_schema": output_model,
                "use_vision": use_vision,
            }

            if self.fast_mode:
                agent_kwargs["flash_mode"] = True  # ç¦ç”¨LLM thinkingä»¥æå‡é€Ÿåº¦
                agent_kwargs["extend_system_message"] = SPEED_OPTIMIZATION_PROMPT
                logger.info("ğŸš€ Fast Mode: flash_mode=True, é€Ÿåº¦ä¼˜åŒ–æç¤ºè¯å·²åº”ç”¨")

            agent = Agent(**agent_kwargs)

            mode_desc = "Fast Modeï¼ˆé€Ÿåº¦ä¼˜å…ˆï¼‰" if self.fast_mode else "æ ‡å‡†æ¨¡å¼ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰"
            logger.info(f"âœ“ AI Agentåˆ›å»ºæˆåŠŸï¼ˆ{mode_desc}ï¼‰")

            # è®°å½•APIè°ƒç”¨
            log_api_call("Browser-Use Agent", request_data={"task": task[:100], "max_steps": max_steps})

            log_step(6, "æ‰§è¡ŒAIè‡ªåŠ¨åŒ–ä»»åŠ¡", timeout=f"{settings.MAX_SCRAPE_TIMEOUT}s")
            logger.info("AIå¼€å§‹æ§åˆ¶æµè§ˆå™¨...")

            # æ‰§è¡Œä»»åŠ¡
            history = await asyncio.wait_for(
                agent.run(max_steps=max_steps),
                timeout=settings.MAX_SCRAPE_TIMEOUT
            )

            # æå–ç»“æœ
            result = history.final_result()

            # æ”¶é›†è®¿é—®çš„URL
            visited_urls = [item.state.url for item in history.history if hasattr(item.state, 'url')]

            logger.info(f"âœ… AIçˆ¬å–æˆåŠŸï¼Œæ‰§è¡Œäº† {len(history.history)} æ­¥")
            logger.info(f"è®¿é—®çš„é¡µé¢: {visited_urls}")

            # è¯¦ç»†è®°å½•æ¯ä¸€æ­¥çš„æ‰§è¡Œæƒ…å†µ
            self._log_agent_steps(history)

            # è®°å½•APIå“åº”
            log_api_call("Browser-Use Agent", response_data=result, status="success")

            return {
                "status": "success",
                "data": result,
                "steps": len(history.history),
                "urls": visited_urls
            }

        except asyncio.TimeoutError:
            logger.error(f"âŒ AIçˆ¬å–è¶…æ—¶: {settings.MAX_SCRAPE_TIMEOUT}ç§’")
            log_api_call("Browser-Use Agent", response_data=None, status="timeout")
            return {
                "status": "timeout",
                "error": f"Task exceeded {settings.MAX_SCRAPE_TIMEOUT}s timeout"
            }
        except Exception as e:
            logger.exception(f"âŒ AIçˆ¬å–å¤±è´¥: {e}")
            log_api_call("Browser-Use Agent", response_data=str(e), status="error")
            return {
                "status": "error",
                "error": str(e)
            }

    async def close(self, force: bool = False):
        """
        å…³é—­æµè§ˆå™¨ä¼šè¯ï¼ˆä¿®å¤èµ„æºæ³„æ¼ï¼‰

        Args:
            force: æ˜¯å¦å¼ºåˆ¶å…³é—­ï¼ˆå¿½ç•¥keep_aliveè®¾ç½®ï¼‰
        """
        # Keep-Aliveæ¨¡å¼ï¼šé™¤éå¼ºåˆ¶å…³é—­ï¼Œå¦åˆ™ä¿æŒä¼šè¯
        if self.keep_alive and not force:
            logger.info("ğŸ”— Keep-Aliveæ¨¡å¼ï¼šä¿æŒæµè§ˆå™¨ä¼šè¯ï¼Œè·³è¿‡å…³é—­")
            return

        if self.browser_session:
            log_step(7, "å…³é—­æµè§ˆå™¨ä¼šè¯")
            try:
                # Browser-Use 0.7.x ä½¿ç”¨ stop() æ–¹æ³•è€Œé close()
                if hasattr(self.browser_session, 'stop'):
                    await self.browser_session.stop()
                    logger.info("âœ“ ä½¿ç”¨ stop() æ–¹æ³•å…³é—­")
                elif hasattr(self.browser_session, 'close'):
                    await self.browser_session.close()
                    logger.info("âœ“ ä½¿ç”¨ close() æ–¹æ³•å…³é—­")

                # ç­‰å¾…èµ„æºé‡Šæ”¾ï¼ˆä¿®å¤ aiohttp è¿æ¥æ³„æ¼ï¼‰
                await asyncio.sleep(0.1)

                logger.info("âœ… æµè§ˆå™¨ä¼šè¯å·²æˆåŠŸå…³é—­")
            except Exception as e:
                # é™çº§ä¸ºè­¦å‘Šï¼Œä¸å½±å“ä¸»æµç¨‹
                logger.warning(f"âš ï¸  å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
            finally:
                self.browser_session = None
                logger.debug("æµè§ˆå™¨ä¼šè¯å¯¹è±¡å·²æ¸…ç©º")

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self._get_browser_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.close()

    async def run_task_chain(
        self,
        tasks: List[str],
        output_model: Optional[type[BaseModel]] = None,
        max_steps_per_task: int = 20
    ) -> List[dict]:
        """
        é“¾å¼æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆä¿æŒæµè§ˆå™¨ä¼šè¯ï¼‰

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼ˆè‡ªç„¶è¯­è¨€æè¿°ï¼‰
            output_model: Pydanticæ¨¡å‹ç±»ï¼ˆå¯é€‰ï¼‰
            max_steps_per_task: æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ­¥éª¤æ•°

        Returns:
            æ¯ä¸ªä»»åŠ¡çš„æ‰§è¡Œç»“æœåˆ—è¡¨

        Example:
            tasks = [
                "æœç´¢'åŒ—äº¬æ•…å®«'",
                "ç‚¹å‡»ç¬¬ä¸€ä¸ªæœç´¢ç»“æœ",
                "æå–é¡µé¢æ ‡é¢˜å’Œæ‘˜è¦"
            ]
            results = await scraper.run_task_chain(tasks)
        """
        if not tasks:
            logger.warning("ä»»åŠ¡åˆ—è¡¨ä¸ºç©º")
            return []

        logger.info(f"ğŸ”— å¼€å§‹é“¾å¼æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡ï¼ˆKeep-Aliveæ¨¡å¼ï¼‰")

        # ç¡®ä¿æµè§ˆå™¨ä¼šè¯å·²å¯åŠ¨
        browser_session = await self._get_browser_session()

        results = []

        for idx, task in enumerate(tasks, 1):
            logger.info(f"ğŸ“ ä»»åŠ¡ {idx}/{len(tasks)}: {task[:50]}...")

            try:
                # ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼šåˆ›å»ºæ–°Agent
                if idx == 1:
                    log_step(5, f"åˆ›å»ºAI Agentï¼ˆä»»åŠ¡{idx}ï¼‰")

                    agent_kwargs = {
                        "task": task,
                        "llm": self.llm,
                        "browser_session": browser_session,
                        "output_model_schema": output_model,
                        "use_vision": True,
                    }

                    if self.fast_mode:
                        agent_kwargs["flash_mode"] = True
                        agent_kwargs["extend_system_message"] = SPEED_OPTIMIZATION_PROMPT

                    self.current_agent = Agent(**agent_kwargs)
                    logger.info(f"âœ“ AI Agentåˆ›å»ºæˆåŠŸï¼ˆä»»åŠ¡{idx}ï¼‰")

                else:
                    # åç»­ä»»åŠ¡ï¼šæ·»åŠ åˆ°ç°æœ‰Agent
                    log_step(5, f"æ·»åŠ æ–°ä»»åŠ¡åˆ°Agentï¼ˆä»»åŠ¡{idx}ï¼‰")
                    self.current_agent.add_new_task(task)
                    logger.info(f"âœ“ ä»»åŠ¡å·²æ·»åŠ åˆ°Agentï¼ˆä»»åŠ¡{idx}ï¼‰")

                # æ‰§è¡Œä»»åŠ¡
                log_step(6, f"æ‰§è¡ŒAIä»»åŠ¡{idx}", timeout=f"{settings.MAX_SCRAPE_TIMEOUT}s")
                history = await asyncio.wait_for(
                    self.current_agent.run(max_steps=max_steps_per_task),
                    timeout=settings.MAX_SCRAPE_TIMEOUT
                )

                # æå–ç»“æœ
                result = history.final_result()
                visited_urls = [item.state.url for item in history.history if hasattr(item.state, 'url')]

                logger.info(f"âœ… ä»»åŠ¡{idx}å®Œæˆï¼Œæ‰§è¡Œäº† {len(history.history)} æ­¥")
                logger.debug(f"è®¿é—®çš„é¡µé¢: {visited_urls}")

                results.append({
                    "task_index": idx,
                    "task": task,
                    "status": "success",
                    "data": result,
                    "steps": len(history.history),
                    "urls": visited_urls
                })

            except asyncio.TimeoutError:
                logger.error(f"âŒ ä»»åŠ¡{idx}è¶…æ—¶: {settings.MAX_SCRAPE_TIMEOUT}ç§’")
                results.append({
                    "task_index": idx,
                    "task": task,
                    "status": "timeout",
                    "error": f"Task exceeded {settings.MAX_SCRAPE_TIMEOUT}s timeout"
                })
                break  # è¶…æ—¶åä¸­æ–­é“¾å¼æ‰§è¡Œ

            except Exception as e:
                logger.exception(f"âŒ ä»»åŠ¡{idx}å¤±è´¥: {e}")
                results.append({
                    "task_index": idx,
                    "task": task,
                    "status": "error",
                    "error": str(e)
                })
                break  # å¤±è´¥åä¸­æ–­é“¾å¼æ‰§è¡Œ

        logger.info(f"ğŸ”— é“¾å¼ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {len(results)}/{len(tasks)} ä¸ªä»»åŠ¡")

        return results

    @staticmethod
    async def run_parallel(
        tasks: List[str],
        output_model: Optional[type[BaseModel]] = None,
        max_steps: int = 20,
        headless: bool = True,
        use_vision: bool = True,
        fast_mode: bool = False
    ) -> List[dict]:
        """
        å¹¶è¡Œæ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡ï¼ˆæ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹ï¼‰

        Args:
            tasks: ä»»åŠ¡æè¿°åˆ—è¡¨ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            output_model: Pydanticæ¨¡å‹ç±»ï¼ˆå¯é€‰ï¼‰
            max_steps: æ¯ä¸ªä»»åŠ¡çš„æœ€å¤§æ­¥éª¤æ•°
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨
            use_vision: æ˜¯å¦å¯ç”¨è§†è§‰èƒ½åŠ›
            fast_mode: æ˜¯å¦å¯ç”¨Fast Mode

        Returns:
            List[dict]: ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {task_index, status, data/error}

        æ³¨æ„ï¼š
            - æ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç‹¬ç«‹çš„æµè§ˆå™¨å®ä¾‹ï¼ˆé¿å…çŠ¶æ€å†²çªï¼‰
            - ä½¿ç”¨ asyncio.gather() å®ç°çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
            - é€‚ç”¨äºå®Œå…¨ç‹¬ç«‹çš„å¤šä¸ªä»»åŠ¡ï¼ˆå¦‚çˆ¬å–å¤šä¸ªç½‘ç«™ï¼‰
            - èµ„æºæ¶ˆè€—è¾ƒé«˜ï¼ˆNä¸ªä»»åŠ¡ = Nä¸ªæµè§ˆå™¨è¿›ç¨‹ï¼‰
        """
        logger.info(f"ğŸš€ å¹¶è¡Œæ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...")

        async def run_single_task(task_index: int, task: str):
            """æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆåˆ›å»ºç‹¬ç«‹æµè§ˆå™¨å®ä¾‹ï¼‰"""
            scraper = None
            try:
                # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«å®ä¾‹
                scraper = BrowserUseScraper(
                    headless=headless,
                    fast_mode=fast_mode,
                    keep_alive=False  # å¹¶è¡Œæ¨¡å¼ä¸ä½¿ç”¨Keep-Alive
                )

                logger.info(f"ğŸ“Œ ä»»åŠ¡ {task_index}: å¯åŠ¨ç‹¬ç«‹æµè§ˆå™¨...")

                # æ‰§è¡Œä»»åŠ¡
                result = await scraper.scrape_with_task(
                    task=task,
                    output_model=output_model,
                    max_steps=max_steps,
                    use_vision=use_vision
                )

                logger.info(f"âœ… ä»»åŠ¡ {task_index}: å®Œæˆ (æ‰§è¡Œäº† {result.get('steps', 0)} æ­¥)")

                return {
                    "task_index": task_index,
                    "task": task,
                    "status": result["status"],
                    "data": result.get("data"),
                    "steps": result.get("steps", 0)
                }

            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡ {task_index} å¤±è´¥: {e}")
                return {
                    "task_index": task_index,
                    "task": task,
                    "status": "error",
                    "error": str(e)
                }

            finally:
                # ç¡®ä¿å…³é—­æµè§ˆå™¨
                if scraper:
                    await scraper.close(force=True)

        # ä½¿ç”¨ asyncio.gather() å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        parallel_tasks = [
            run_single_task(idx, task)
            for idx, task in enumerate(tasks, 1)
        ]

        results = await asyncio.gather(*parallel_tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        final_results = []
        for idx, result in enumerate(results, 1):
            if isinstance(result, Exception):
                logger.error(f"âŒ ä»»åŠ¡ {idx} å¼‚å¸¸: {result}")
                final_results.append({
                    "task_index": idx,
                    "task": tasks[idx - 1],
                    "status": "exception",
                    "error": str(result)
                })
            else:
                final_results.append(result)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in final_results if r["status"] == "success")
        logger.info(
            f"ğŸ å¹¶è¡Œä»»åŠ¡å®Œæˆ: {success_count}/{len(tasks)} ä¸ªæˆåŠŸ"
        )

        return final_results
