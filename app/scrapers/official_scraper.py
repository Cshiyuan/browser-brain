"""åŸºäºBrowser-Useçš„å®˜ç½‘AIçˆ¬è™«"""
from typing import List, Optional

from app.scrapers.browser_use_scraper import BrowserUseScraper
from app.scrapers.models import OfficialInfoOutput
from app.models.attraction import OfficialInfo, XHSNote
from app.utils.logger import setup_logger

logger = setup_logger(__name__)


class OfficialScraper(BrowserUseScraper):
    """åŸºäºBrowser-Useçš„å®˜ç½‘AIçˆ¬è™«"""

    async def get_official_info(
        self,
        attraction_name: str,
        xhs_notes: List[XHSNote]
    ) -> Optional[OfficialInfo]:
        """
        ä½¿ç”¨AIè·å–æ™¯ç‚¹å®˜æ–¹ä¿¡æ¯

        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä»å°çº¢ä¹¦ç¬”è®°ä¸­æå–çš„é“¾æ¥
        2. å¦‚æœæ²¡æœ‰,ä½¿ç”¨æœç´¢å¼•æ“æŸ¥æ‰¾
        3. AIè‡ªåŠ¨è®¿é—®å®˜ç½‘å¹¶æå–ä¿¡æ¯

        Args:
            attraction_name: æ™¯ç‚¹åç§°
            xhs_notes: å°çº¢ä¹¦ç¬”è®°åˆ—è¡¨

        Returns:
            å®˜æ–¹ä¿¡æ¯å¯¹è±¡
        """
        logger.info(f"========== å¼€å§‹å®˜ç½‘ä¿¡æ¯çˆ¬å– ==========")
        logger.info(f"ç›®æ ‡æ™¯ç‚¹: {attraction_name}, å‚è€ƒç¬”è®°æ•°: {len(xhs_notes)}")
        logger.info(f"ğŸ“ STEP 1: å‡†å¤‡å®˜ç½‘ä¿¡æ¯çˆ¬å–ä»»åŠ¡ | attraction={attraction_name}")

        # ä»å°çº¢ä¹¦ç¬”è®°ä¸­æ”¶é›†æ‰€æœ‰é“¾æ¥
        collected_links = []
        for note in xhs_notes:
            # å¦‚æœæœ‰ url å­—æ®µ,æ·»åŠ åˆ°é“¾æ¥åˆ—è¡¨
            if note.url:
                collected_links.append(note.url)

        # å»é‡
        collected_links = list(set(collected_links))
        logger.info(f"ä»å°çº¢ä¹¦ç¬”è®°ä¸­æ”¶é›†åˆ° {len(collected_links)} ä¸ªé“¾æ¥")
        logger.debug(f"æ”¶é›†çš„é“¾æ¥: {collected_links[:5]}")

        # æ„å»ºAIä»»åŠ¡
        if collected_links:
            task = f"""
ä»»åŠ¡ï¼šæŸ¥æ‰¾å¹¶æå–"{attraction_name}"çš„å®˜æ–¹ä¿¡æ¯

å·²çŸ¥ä¿¡æ¯ï¼š
å°çº¢ä¹¦ç”¨æˆ·æä¾›çš„å¯èƒ½çš„å®˜æ–¹é“¾æ¥ï¼š
{chr(10).join(['- ' + link for link in collected_links[:5]])}

å…·ä½“æ­¥éª¤ï¼š
1. é¦–å…ˆéªŒè¯ä¸Šè¿°é“¾æ¥ï¼Œæ‰¾åˆ°çœŸæ­£çš„å®˜æ–¹ç½‘ç«™
2. å¦‚æœä¸Šè¿°é“¾æ¥éƒ½ä¸æ˜¯å®˜ç½‘ï¼Œåˆ™ä½¿ç”¨ç™¾åº¦æˆ–Googleæœç´¢"{attraction_name} å®˜ç½‘"
3. è®¿é—®å®˜æ–¹ç½‘ç«™
4. æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
   - å®˜æ–¹ç½‘ç«™URL
   - å¼€æ”¾æ—¶é—´/è¥ä¸šæ—¶é—´
   - é—¨ç¥¨ä»·æ ¼ï¼ˆæˆäººç¥¨ã€å­¦ç”Ÿç¥¨ã€å„¿ç«¥ç¥¨ç­‰ï¼‰
   - é¢„è®¢æ–¹å¼ï¼ˆç½‘ä¸Šé¢„è®¢ã€ç°åœºè´­ç¥¨ã€å…¬ä¼—å·é¢„çº¦ç­‰ï¼‰
   - è¯¦ç»†åœ°å€
   - è”ç³»ç”µè¯
   - æ™¯ç‚¹ç®€ä»‹/æè¿°

é‡è¦æç¤ºï¼š
- ç¡®ä¿è®¿é—®çš„æ˜¯å®˜æ–¹ç½‘ç«™ï¼Œè€Œä¸æ˜¯æ—…æ¸¸å¹³å°
- å¦‚æœå®˜ç½‘ä¿¡æ¯ä¸å…¨ï¼Œå¯ä»¥è®¿é—®æ™¯ç‚¹çš„å¾®ä¿¡å…¬ä¼—å·æˆ–å®˜æ–¹å°ç¨‹åº
- è¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®
"""
        else:
            task = f"""
ä»»åŠ¡ï¼šæŸ¥æ‰¾å¹¶æå–"{attraction_name}"çš„å®˜æ–¹ä¿¡æ¯

å…·ä½“æ­¥éª¤ï¼š
1. è®¿é—®ç™¾åº¦æœç´¢ https://www.baidu.com
2. ç­‰å¾…é¡µé¢åŠ è½½(2-3ç§’)
3. åœ¨æœç´¢æ¡†è¾“å…¥ï¼š"{attraction_name} å®˜ç½‘"
4. ç‚¹å‡»æœç´¢æˆ–æŒ‰å›è½¦
5. ç­‰å¾…æœç´¢ç»“æœåŠ è½½
6. è¯†åˆ«å®˜æ–¹ç½‘ç«™ï¼ˆä¼˜å…ˆé€‰æ‹©.gov.cnæˆ–åŒ…å«æ™¯ç‚¹åç§°çš„åŸŸåï¼‰
7. ç‚¹å‡»è¿›å…¥å®˜æ–¹ç½‘ç«™
8. ç­‰å¾…å®˜ç½‘åŠ è½½å®Œæˆ
9. æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
   - å®˜æ–¹ç½‘ç«™URL
   - å¼€æ”¾æ—¶é—´/è¥ä¸šæ—¶é—´
   - é—¨ç¥¨ä»·æ ¼ï¼ˆæˆäººç¥¨ã€å­¦ç”Ÿç¥¨ã€å„¿ç«¥ç¥¨ç­‰ï¼‰
   - é¢„è®¢æ–¹å¼ï¼ˆç½‘ä¸Šé¢„è®¢ã€ç°åœºè´­ç¥¨ã€å…¬ä¼—å·é¢„çº¦ç­‰ï¼‰
   - è¯¦ç»†åœ°å€
   - è”ç³»ç”µè¯
   - æ™¯ç‚¹ç®€ä»‹/æè¿°

é‡è¦æç¤ºï¼š
- åƒçœŸå®ç”¨æˆ·ä¸€æ ·æ“ä½œï¼Œæ¯æ­¥ä¹‹é—´ç•™æœ‰é—´éš”
- ä¼˜å…ˆé€‰æ‹©.gov.cnæˆ–æ™¯ç‚¹å®˜æ–¹åŸŸå
- é¿å…æ‰“å¼€ç¬¬ä¸‰æ–¹æ—…æ¸¸å¹³å°ï¼ˆå¦‚æºç¨‹ã€ç¾å›¢ã€å»å“ªå„¿ç­‰ï¼‰
- å¦‚æœé‡åˆ°å¼¹çª—æˆ–å¹¿å‘Šï¼Œå…ˆå…³é—­å®ƒä»¬
- å¦‚æœç™¾åº¦ä¸å¯ç”¨ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨Bingæœç´¢
- è¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®
"""

        # ä½¿ç”¨AIæ‰§è¡Œçˆ¬å–
        logger.info("ğŸ“ STEP 2: è°ƒç”¨Browser-Use AIæ‰§è¡Œå®˜ç½‘ä¿¡æ¯çˆ¬å–")
        result = await self.scrape_with_task(
            task=task,
            output_model=OfficialInfoOutput,
            max_steps=25
        )

        logger.info(f"ğŸ“ STEP 3: å¤„ç†AIè¿”å›ç»“æœ | status={result['status']}")

        if result["status"] != "success":
            logger.warning(f"âš ï¸  AIè·å–å®˜ç½‘ä¿¡æ¯å¤±è´¥: {result.get('error', 'Unknown error')}")
            logger.warning(f"æ‰§è¡Œæ­¥éª¤æ•°: {result.get('steps', 0)}, è®¿é—®çš„URL: {result.get('urls', [])}")
            return None

        # è½¬æ¢ä¸ºOfficialInfoå¯¹è±¡
        data = result["data"]

        # å¤„ç† AI Agent å¤±è´¥è¿”å› None æˆ–å­—ç¬¦ä¸²çš„æƒ…å†µ
        if data is None or isinstance(data, str):
            logger.warning(f"âš ï¸  AIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸æˆ–æ— æ•°æ®: {type(data)}")
            logger.debug(f"åŸå§‹è¿”å›æ•°æ®: {data}")
            return None

        logger.info(f"AIæˆåŠŸè¿”å›å®˜ç½‘ä¿¡æ¯æ•°æ®")
        logger.info("ğŸ“ STEP 4: è½¬æ¢æ•°æ®ä¸ºOfficialInfoå¯¹è±¡")

        official_info = OfficialInfo(
            name=attraction_name,
            website=data.website or "",
            opening_hours=data.opening_hours or "",
            ticket_price=data.ticket_price or "",
            booking_info=data.booking_method or "",
            address=data.address or "",
            phone=data.phone or "",
            description=data.description or ""
        )

        logger.info(f"âœ… æˆåŠŸè·å–å®˜ç½‘ä¿¡æ¯: {attraction_name}")
        logger.debug(f"å®˜ç½‘: {official_info.website}")
        logger.debug(f"åœ°å€: {official_info.address}")
        logger.debug(f"é—¨ç¥¨: {official_info.ticket_price}")
        logger.info(f"========== å®˜ç½‘ä¿¡æ¯çˆ¬å–å®Œæˆ ==========")
        return official_info

    async def scrape(
        self,
        attraction_name: str,
        xhs_notes: List[XHSNote]
    ) -> Optional[OfficialInfo]:
        """
        å®ç°åŸºç±»çš„æŠ½è±¡æ–¹æ³•

        Args:
            attraction_name: æ™¯ç‚¹åç§°
            xhs_notes: å°çº¢ä¹¦ç¬”è®°åˆ—è¡¨

        Returns:
            å®˜æ–¹ä¿¡æ¯
        """
        return await self.get_official_info(attraction_name, xhs_notes)
